# Êï∞ÊçÆÊåñÊéò - ÂΩ¢ÂºèÂåñÁêÜËÆ∫‰∏éHaskellÂÆûÁé∞

## üìã Ê¶ÇËø∞

Êï∞ÊçÆÊåñÊéòÊòØ‰ªéÂ§ßÈáèÊï∞ÊçÆ‰∏≠ÂèëÁé∞ÊúâÁî®Ê®°Âºè„ÄÅÂÖ≥ËÅîÂíåÁü•ËØÜÁöÑËøáÁ®ã„ÄÇÊú¨ÊñáÊ°£‰ªéÂΩ¢ÂºèÂåñËßíÂ∫¶Âª∫Á´ãÊï∞ÊçÆÊåñÊéòÁöÑÂÆåÊï¥ÁêÜËÆ∫‰ΩìÁ≥ªÔºåÂåÖÊã¨ÂàÜÁ±ª„ÄÅËÅöÁ±ª„ÄÅÂÖ≥ËÅîËßÑÂàôÊåñÊéòÁ≠âÊ†∏ÂøÉÁÆóÊ≥ï„ÄÇ

## üéØ Ê†∏ÂøÉÊ¶ÇÂøµ

### 1. Êï∞ÊçÆÊåñÊéòÂü∫Á°Ä

#### ÂΩ¢ÂºèÂåñÂÆö‰πâ

**ÂÆö‰πâ 1.1 (Êï∞ÊçÆÈõÜ)** Êï∞ÊçÆÈõÜ $D$ ÊòØ‰∏Ä‰∏™ÂÖÉÁªÑ $(X, Y, f)$ÔºåÂÖ∂‰∏≠Ôºö

- $X = \{x_1, \ldots, x_n\}$ ÊòØÁâπÂæÅÁ©∫Èó¥
- $Y$ ÊòØÁõÆÊ†áÁ©∫Èó¥ÔºàÁõëÁù£Â≠¶‰π†ÔºâÊàñÁ©∫ÈõÜÔºàÊó†ÁõëÁù£Â≠¶‰π†Ôºâ
- $f: X \to Y$ ÊòØÁõÆÊ†áÂáΩÊï∞ÔºàÁõëÁù£Â≠¶‰π†Ôºâ

**ÂÆö‰πâ 1.2 (Ê®°Âºè)** Ê®°Âºè $P$ ÊòØÊï∞ÊçÆÈõÜ $D$ ‰∏≠Êª°Ë∂≥ÁâπÂÆöÊù°‰ª∂ÁöÑÊï∞ÊçÆÂ≠êÈõÜÔºö
$$P = \{x \in D : \phi(x)\}$$
ÂÖ∂‰∏≠ $\phi$ ÊòØÊ®°ÂºèÊèèËø∞ÂáΩÊï∞„ÄÇ

#### HaskellÂÆûÁé∞

```haskell
{-# LANGUAGE GADTs, TypeFamilies, DataKinds #-}
{-# LANGUAGE FlexibleContexts, FlexibleInstances #-}

module DataMining.Core where

import Data.Set (Set)
import qualified Data.Set as Set
import Data.Map (Map)
import qualified Data.Map as Map
import Data.List (nub)

-- ÁâπÂæÅÂêëÈáè
type FeatureVector a = [a]

-- Êï∞ÊçÆÈõÜ
data Dataset a b where
  SupervisedDataset :: 
    { features :: [FeatureVector a]
    , labels :: [b]
    } -> Dataset a b
  UnsupervisedDataset :: 
    { features :: [FeatureVector a]
    } -> Dataset a b

-- Ê®°ÂºèÊèèËø∞ÂáΩÊï∞
type PatternDescription a = FeatureVector a -> Bool

-- Ê®°Âºè
data Pattern a where
  Pattern :: 
    { description :: PatternDescription a
    , support :: Double
    , confidence :: Double
    } -> Pattern a

-- ËÆ°ÁÆóÊîØÊåÅÂ∫¶
support :: (Eq a) => PatternDescription a -> [FeatureVector a] -> Double
support pattern data_ =
  let matching = filter pattern data_
      total = length data_
  in fromIntegral (length matching) / fromIntegral total

-- ËÆ°ÁÆóÁΩÆ‰ø°Â∫¶ÔºàÁî®‰∫éÂÖ≥ËÅîËßÑÂàôÔºâ
confidence :: (Eq a) => 
  PatternDescription a -> 
  PatternDescription a -> 
  [FeatureVector a] -> 
  Double
confidence antecedent consequent data_ =
  let antecedentSupport = support antecedent data_
      bothSupport = support (\x -> antecedent x && consequent x) data_
  in if antecedentSupport > 0 then bothSupport / antecedentSupport else 0
```

### 2. ÂàÜÁ±ªÁÆóÊ≥ï

#### ÂΩ¢ÂºèÂåñÂÆö‰πâ

**ÂÆö‰πâ 1.3 (ÂàÜÁ±ªÈóÆÈ¢ò)** ÁªôÂÆöËÆ≠ÁªÉÈõÜ $T = \{(x_i, y_i)\}_{i=1}^n$ÔºåÂ≠¶‰π†ÂáΩÊï∞ $f: X \to Y$ ‰ΩøÂæóÔºö
$$\forall (x, y) \in T, f(x) = y$$

**ÂÆö‰πâ 1.4 (ÂÜ≥Á≠ñÊ†ë)** ÂÜ≥Á≠ñÊ†ëÊòØ‰∏Ä‰∏™ÈÄíÂΩíÁªìÊûÑÔºö

- Âè∂ËäÇÁÇπÔºöÂåÖÂê´Á±ªÂà´Ê†áÁ≠æ
- ÂÜÖÈÉ®ËäÇÁÇπÔºöÂåÖÂê´ÂàÜË£ÇÊù°‰ª∂ÂíåÂ≠êËäÇÁÇπ

#### HaskellÂÆûÁé∞

```haskell
module DataMining.Classification where

import DataMining.Core
import Data.List (maximumBy, group, sort)
import Data.Ord (comparing)
import Data.Map (Map)
import qualified Data.Map as Map

-- ÂÜ≥Á≠ñÊ†ë
data DecisionTree a b where
  Leaf :: b -> DecisionTree a b
  Node :: 
    { splitFeature :: Int
    , splitValue :: a
    , leftChild :: DecisionTree a b
    , rightChild :: DecisionTree a b
    } -> DecisionTree a b

-- ‰ø°ÊÅØÁÜµ
entropy :: (Eq b) => [b] -> Double
entropy labels =
  let total = length labels
      counts = Map.fromListWith (+) [(label, 1) | label <- labels]
      probabilities = [fromIntegral count / fromIntegral total | count <- Map.elems counts]
  in -sum [p * logBase 2 p | p <- probabilities, p > 0]

-- ‰ø°ÊÅØÂ¢ûÁõä
informationGain :: (Ord a, Eq b) => 
  Int ->  -- ÁâπÂæÅÁ¥¢Âºï
  a ->    -- ÂàÜË£ÇÂÄº
  [(FeatureVector a, b)] ->  -- ËÆ≠ÁªÉÊï∞ÊçÆ
  Double
informationGain featureIndex splitValue data_ =
  let parentEntropy = entropy [label | (_, label) <- data_]
      leftData = [(features, label) | (features, label) <- data_, 
                                     features !! featureIndex <= splitValue]
      rightData = [(features, label) | (features, label) <- data_, 
                                      features !! featureIndex > splitValue]
      leftEntropy = entropy [label | (_, label) <- leftData]
      rightEntropy = entropy [label | (_, label) <- rightData]
      leftWeight = fromIntegral (length leftData) / fromIntegral (length data_)
      rightWeight = fromIntegral (length rightData) / fromIntegral (length data_)
  in parentEntropy - (leftWeight * leftEntropy + rightWeight * rightEntropy)

-- ÊûÑÂª∫ÂÜ≥Á≠ñÊ†ë
buildDecisionTree :: (Ord a, Eq b) => [(FeatureVector a, b)] -> DecisionTree a b
buildDecisionTree data_
  | allSameLabels = Leaf (snd $ head data_)
  | otherwise = 
      let bestSplit = findBestSplit data_
          leftData = [(features, label) | (features, label) <- data_, 
                                         features !! (fst bestSplit) <= snd bestSplit]
          rightData = [(features, label) | (features, label) <- data_, 
                                          features !! (fst bestSplit) > snd bestSplit]
      in Node (fst bestSplit) (snd bestSplit) 
             (buildDecisionTree leftData) 
             (buildDecisionTree rightData)
  where
    allSameLabels = length (nub [label | (_, label) <- data_]) == 1

-- ÊâæÂà∞ÊúÄ‰Ω≥ÂàÜË£ÇÁÇπ
findBestSplit :: (Ord a, Eq b) => [(FeatureVector a, b)] -> (Int, a)
findBestSplit data_ =
  let numFeatures = length (fst $ head data_)
      featureValues = [[features !! i | (features, _) <- data_] | i <- [0..numFeatures-1]]
      splits = [(i, value) | i <- [0..numFeatures-1], 
                            value <- nub (featureValues !! i)]
      gains = [(split, informationGain (fst split) (snd split) data_) | split <- splits]
  in maximumBy (comparing snd) gains

-- È¢ÑÊµã
predict :: (Ord a) => DecisionTree a b -> FeatureVector a -> b
predict (Leaf label) _ = label
predict (Node featureIndex splitValue left right) features =
  if features !! featureIndex <= splitValue
  then predict left features
  else predict right features

-- Êú¥Á¥†Ë¥ùÂè∂ÊñØÂàÜÁ±ªÂô®
data NaiveBayes a b = NaiveBayes
  { classProbabilities :: Map b Double
  , featureProbabilities :: Map (b, Int, a) Double
  } deriving (Show)

-- ËÆ≠ÁªÉÊú¥Á¥†Ë¥ùÂè∂ÊñØ
trainNaiveBayes :: (Ord a, Eq b) => [(FeatureVector a, b)] -> NaiveBayes a b
trainNaiveBayes data_ =
  let total = length data_
      classCounts = Map.fromListWith (+) [(label, 1) | (_, label) <- data_]
      classProbs = Map.map (\count -> fromIntegral count / fromIntegral total) classCounts
      
      featureProbs = Map.fromListWith (+) 
        [(label, featureIndex, featureValue, 1) | 
         (features, label) <- data_, 
         (featureIndex, featureValue) <- zip [0..] features]
      
      featureProbsNormalized = Map.mapWithKey 
        (\(label, featureIndex, featureValue) count ->
           count / fromIntegral (classCounts Map.! label))
        featureProbs
  in NaiveBayes classProbs featureProbsNormalized

-- Êú¥Á¥†Ë¥ùÂè∂ÊñØÈ¢ÑÊµã
predictNaiveBayes :: (Ord a) => NaiveBayes a b -> FeatureVector a -> b
predictNaiveBayes model features =
  let classScores = Map.mapWithKey (\class_ classProb ->
        let featureScore = product [Map.findWithDefault 0.1 (class_, i, feature) model.featureProbabilities 
                                  | (i, feature) <- zip [0..] features]
        in classProb * featureScore) model.classProbabilities
  in fst $ maximumBy (comparing snd) (Map.toList classScores)
```

### 3. ËÅöÁ±ªÁÆóÊ≥ï

#### ÂΩ¢ÂºèÂåñÂÆö‰πâ

**ÂÆö‰πâ 1.5 (ËÅöÁ±ª)** ËÅöÁ±ªÊòØÂ∞ÜÊï∞ÊçÆÈõÜ $D$ ÂàíÂàÜ‰∏∫ $k$ ‰∏™‰∏çÁõ∏‰∫§Â≠êÈõÜ $C_1, \ldots, C_k$ ÁöÑËøáÁ®ãÔºö
$$D = \bigcup_{i=1}^k C_i, \quad C_i \cap C_j = \emptyset \text{ for } i \neq j$$

**ÂÆö‰πâ 1.6 (K-meansÁõÆÊ†áÂáΩÊï∞)** K-meansÊúÄÂ∞èÂåñ‰ª•‰∏ãÁõÆÊ†áÂáΩÊï∞Ôºö
$$J = \sum_{i=1}^k \sum_{x \in C_i} \|x - \mu_i\|^2$$
ÂÖ∂‰∏≠ $\mu_i$ ÊòØËÅöÁ±ª $C_i$ ÁöÑ‰∏≠ÂøÉ„ÄÇ

#### HaskellÂÆûÁé∞

```haskell
module DataMining.Clustering where

import DataMining.Core
import Data.List (minimumBy, maximumBy, nub)
import Data.Ord (comparing)
import Data.Map (Map)
import qualified Data.Map as Map

-- ËÅöÁ±ªÁªìÊûú
type Cluster a = [FeatureVector a]
type Clustering a = [Cluster a]

-- Ê¨ßÂá†ÈáåÂæóË∑ùÁ¶ª
euclideanDistance :: (Num a) => FeatureVector a -> FeatureVector a -> Double
euclideanDistance x y = sqrt $ sum [(xi - yi)^2 | (xi, yi) <- zip x y]

-- K-meansËÅöÁ±ª
kMeans :: (Num a, Fractional a) => 
  Int ->  -- ËÅöÁ±ªÊï∞
  [FeatureVector a] ->  -- Êï∞ÊçÆ
  Clustering a
kMeans k data_ =
  let initialCentroids = take k data_  -- ÁÆÄÂåñÔºö‰ΩøÁî®Ââçk‰∏™ÁÇπ‰Ωú‰∏∫ÂàùÂßã‰∏≠ÂøÉ
      finalCentroids = converge data_ initialCentroids
  in assignToClusters data_ finalCentroids

-- Êî∂ÊïõÂà∞ÊúÄÁªà‰∏≠ÂøÉ
converge :: (Num a, Fractional a) => 
  [FeatureVector a] -> 
  [FeatureVector a] -> 
  [FeatureVector a]
converge data_ centroids =
  let newCentroids = updateCentroids data_ centroids
  in if centroids == newCentroids
     then centroids
     else converge data_ newCentroids

-- Êõ¥Êñ∞ËÅöÁ±ª‰∏≠ÂøÉ
updateCentroids :: (Num a, Fractional a) => 
  [FeatureVector a] -> 
  [FeatureVector a] -> 
  [FeatureVector a]
updateCentroids data_ centroids =
  let assignments = [findClosest centroid data_ | centroid <- centroids]
      newCentroids = [computeCentroid cluster | cluster <- assignments]
  in newCentroids

-- ÊâæÂà∞ÊúÄËøëÁöÑËÅöÁ±ª
findClosest :: (Num a) => 
  FeatureVector a -> 
  [FeatureVector a] -> 
  Cluster a
findClosest centroid data_ =
  let distances = [(point, euclideanDistance centroid point) | point <- data_]
      minDistance = minimum [distance | (_, distance) <- distances]
  in [point | (point, distance) <- distances, distance == minDistance]

-- ËÆ°ÁÆóËÅöÁ±ª‰∏≠ÂøÉ
computeCentroid :: (Num a, Fractional a) => Cluster a -> FeatureVector a
computeCentroid cluster =
  let numFeatures = length (head cluster)
      featureSums = [sum [point !! i | point <- cluster] | i <- [0..numFeatures-1]]
      clusterSize = fromIntegral (length cluster)
  in [sum / clusterSize | sum <- featureSums]

-- ÂàÜÈÖçÊï∞ÊçÆÁÇπÂà∞ËÅöÁ±ª
assignToClusters :: (Num a) => 
  [FeatureVector a] -> 
  [FeatureVector a] -> 
  Clustering a
assignToClusters data_ centroids =
  let assignments = [assignToCluster point centroids | point <- data_]
      clusters = Map.fromListWith (++) [(i, [point]) | (point, i) <- zip data_ assignments]
  in [clusters Map.! i | i <- [0..length centroids-1]]

-- ÂàÜÈÖçÂçï‰∏™ÁÇπÂà∞ËÅöÁ±ª
assignToCluster :: (Num a) => 
  FeatureVector a -> 
  [FeatureVector a] -> 
  Int
assignToCluster point centroids =
  let distances = [euclideanDistance point centroid | centroid <- centroids]
  in fst $ minimumBy (comparing snd) (zip [0..] distances)

-- Â±ÇÊ¨°ËÅöÁ±ª
data HierarchicalCluster a = 
  Leaf a |
  Node [HierarchicalCluster a] Double
  deriving (Show)

-- ÂçïÈìæÊé•Â±ÇÊ¨°ËÅöÁ±ª
singleLinkClustering :: (Num a) => 
  [FeatureVector a] -> 
  HierarchicalCluster a
singleLinkClustering data_
  | length data_ == 1 = Leaf (head data_)
  | otherwise =
      let (i, j, distance) = findClosestPair data_
          cluster1 = data_ !! i
          cluster2 = data_ !! j
          remaining = [data_ !! k | k <- [0..length data_-1], k /= i, k /= j]
          mergedCluster = [cluster1, cluster2]
      in Node [singleLinkClustering (mergedCluster ++ remaining)] distance

-- ÊâæÂà∞ÊúÄËøëÁöÑÁÇπÂØπ
findClosestPair :: (Num a) => [FeatureVector a] -> (Int, Int, Double)
findClosestPair data_ =
  let pairs = [(i, j, euclideanDistance (data_ !! i) (data_ !! j)) 
               | i <- [0..length data_-1], j <- [i+1..length data_-1]]
  in minimumBy (comparing (\(_, _, d) -> d)) pairs
```

## üî¨ È´òÁ∫ßÊï∞ÊçÆÊåñÊéòÊäÄÊúØ

### 1. ÂÖ≥ËÅîËßÑÂàôÊåñÊéò

#### ÂΩ¢ÂºèÂåñÂÆö‰πâ

**ÂÆö‰πâ 1.7 (ÂÖ≥ËÅîËßÑÂàô)** ÂÖ≥ËÅîËßÑÂàôÊòØÂΩ¢Â¶Ç $A \Rightarrow B$ ÁöÑË°®ËææÂºèÔºåÂÖ∂‰∏≠ $A, B$ ÊòØÈ°πÈõÜ„ÄÇ

**ÂÆö‰πâ 1.8 (ÊîØÊåÅÂ∫¶ÂíåÁΩÆ‰ø°Â∫¶)** ÂÖ≥ËÅîËßÑÂàô $A \Rightarrow B$ ÁöÑÔºö

- ÊîØÊåÅÂ∫¶Ôºö$support(A \Rightarrow B) = P(A \cup B)$
- ÁΩÆ‰ø°Â∫¶Ôºö$confidence(A \Rightarrow B) = P(B|A) = \frac{P(A \cup B)}{P(A)}$

#### HaskellÂÆûÁé∞

```haskell
module DataMining.AssociationRules where

import DataMining.Core
import Data.Set (Set)
import qualified Data.Set as Set
import Data.List (subsequences, nub)

-- È°πÈõÜ
type Itemset a = Set a

-- ÂÖ≥ËÅîËßÑÂàô
data AssociationRule a = AssociationRule
  { antecedent :: Itemset a
  , consequent :: Itemset a
  , support :: Double
  , confidence :: Double
  } deriving (Show)

-- ‰∫ãÂä°Êï∞ÊçÆÂ∫ì
type Transaction a = Itemset a
type TransactionDatabase a = [Transaction a]

-- AprioriÁÆóÊ≥ï
apriori :: (Ord a) => 
  Double ->  -- ÊúÄÂ∞èÊîØÊåÅÂ∫¶
  TransactionDatabase a -> 
  [Itemset a]
apriori minSupport database =
  let allItems = Set.unions database
      frequent1Itemsets = filter (\item -> 
        support (Set.singleton item) database >= minSupport) (Set.toList allItems)
  in aprioriGen frequent1Itemsets database minSupport

-- AprioriÁîüÊàê
aprioriGen :: (Ord a) => 
  [Itemset a] -> 
  TransactionDatabase a -> 
  Double -> 
  [Itemset a]
aprioriGen candidates database minSupport =
  let frequent = filter (\candidate -> 
        support candidate database >= minSupport) candidates
  in if null frequent
     then []
     else frequent ++ aprioriGen (generateCandidates frequent) database minSupport

-- ÁîüÊàêÂÄôÈÄâÈõÜ
generateCandidates :: (Ord a) => [Itemset a] -> [Itemset a]
generateCandidates frequent =
  let k = Set.size (head frequent)
      candidates = [Set.union set1 set2 | 
                   set1 <- frequent, set2 <- frequent,
                   Set.size (Set.intersection set1 set2) == k - 1]
  in nub candidates

-- ËÆ°ÁÆóÊîØÊåÅÂ∫¶
support :: (Ord a) => Itemset a -> TransactionDatabase a -> Double
support itemset database =
  let count = length [transaction | transaction <- database, 
                                  itemset `Set.isSubsetOf` transaction]
      total = length database
  in fromIntegral count / fromIntegral total

-- ÁîüÊàêÂÖ≥ËÅîËßÑÂàô
generateAssociationRules :: (Ord a) => 
  Double ->  -- ÊúÄÂ∞èÁΩÆ‰ø°Â∫¶
  [Itemset a] -> 
  TransactionDatabase a -> 
  [AssociationRule a]
generateAssociationRules minConfidence frequentItemsets database =
  concat [generateRulesFromItemset itemset database minConfidence 
          | itemset <- frequentItemsets, Set.size itemset > 1]

-- ‰ªéÈ°πÈõÜÁîüÊàêËßÑÂàô
generateRulesFromItemset :: (Ord a) => 
  Itemset a -> 
  TransactionDatabase a -> 
  Double -> 
  [AssociationRule a]
generateRulesFromItemset itemset database minConfidence =
  let subsets = [subset | subset <- Set.toList (powerSet itemset), 
                         not (Set.null subset), subset /= itemset]
      rules = [AssociationRule 
                { antecedent = antecedent
                , consequent = itemset `Set.difference` antecedent
                , support = support itemset database
                , confidence = confidence antecedent (itemset `Set.difference` antecedent) database
                } | antecedent <- subsets]
  in filter (\rule -> rule.confidence >= minConfidence) rules

-- ËÆ°ÁÆóÁΩÆ‰ø°Â∫¶
confidence :: (Ord a) => 
  Itemset a -> 
  Itemset a -> 
  TransactionDatabase a -> 
  Double
confidence antecedent consequent database =
  let antecedentSupport = support antecedent database
      unionSupport = support (antecedent `Set.union` consequent) database
  in if antecedentSupport > 0 then unionSupport / antecedentSupport else 0

-- ÂπÇÈõÜ
powerSet :: (Ord a) => Set a -> Set (Set a)
powerSet set = Set.fromList [Set.fromList subset | subset <- subsequences (Set.toList set)]
```

### 2. ÂºÇÂ∏∏Ê£ÄÊµã

#### ÂΩ¢ÂºèÂåñÂÆö‰πâ

**ÂÆö‰πâ 1.9 (ÂºÇÂ∏∏)** ÂºÇÂ∏∏ÊòØÊï∞ÊçÆÈõÜ‰∏≠‰∏éÂ§ßÂ§öÊï∞Êï∞ÊçÆÁÇπÊòæËëó‰∏çÂêåÁöÑÊï∞ÊçÆÁÇπ„ÄÇ

**ÂÆö‰πâ 1.10 (Â±ÄÈÉ®ÂºÇÂ∏∏Âõ†Â≠ê)** Â±ÄÈÉ®ÂºÇÂ∏∏Âõ†Â≠êÂÆö‰πâ‰∏∫Ôºö
$$LOF(p) = \frac{\sum_{o \in N_k(p)} \frac{lrd_k(o)}{lrd_k(p)}}{|N_k(p)|}$$

#### HaskellÂÆûÁé∞

```haskell
module DataMining.AnomalyDetection where

import DataMining.Core
import DataMining.Clustering
import Data.List (sort, minimumBy)
import Data.Ord (comparing)

-- ÂºÇÂ∏∏Ê£ÄÊµãÁªìÊûú
data AnomalyScore a = AnomalyScore
  { point :: FeatureVector a
  , score :: Double
  , isAnomaly :: Bool
  } deriving (Show)

-- Âü∫‰∫éË∑ùÁ¶ªÁöÑÂºÇÂ∏∏Ê£ÄÊµã
distanceBasedAnomaly :: (Num a) => 
  Double ->  -- ÂºÇÂ∏∏ÈòàÂÄº
  [FeatureVector a] -> 
  [AnomalyScore a]
distanceBasedAnomaly threshold data_ =
  let scores = [AnomalyScore point (averageDistance point data_) False | point <- data_]
      maxScore = maximum [score | AnomalyScore _ score _ <- scores]
      normalizedScores = [score { score = score.score / maxScore, 
                                 isAnomaly = score.score / maxScore > threshold } | score <- scores]
  in normalizedScores

-- ËÆ°ÁÆóÂπ≥ÂùáË∑ùÁ¶ª
averageDistance :: (Num a) => FeatureVector a -> [FeatureVector a] -> Double
averageDistance point data_ =
  let distances = [euclideanDistance point other | other <- data_, other /= point]
  in sum distances / fromIntegral (length distances)

-- Â±ÄÈÉ®ÂºÇÂ∏∏Âõ†Â≠ê
localOutlierFactor :: (Num a) => 
  Int ->  -- kÂÄº
  Double ->  -- ÂºÇÂ∏∏ÈòàÂÄº
  [FeatureVector a] -> 
  [AnomalyScore a]
localOutlierFactor k threshold data_ =
  let lofScores = [computeLOF point k data_ | point <- data_]
      maxLOF = maximum lofScores
      normalizedScores = [AnomalyScore point (score / maxLOF) (score / maxLOF > threshold) 
                         | (point, score) <- zip data_ lofScores]
  in normalizedScores

-- ËÆ°ÁÆóLOF
computeLOF :: (Num a) => FeatureVector a -> Int -> [FeatureVector a] -> Double
computeLOF point k data_ =
  let neighbors = kNearestNeighbors point k data_
      lrdPoint = localReachabilityDensity point k data_
      neighborLRDs = [localReachabilityDensity neighbor k data_ | neighbor <- neighbors]
  in sum neighborLRDs / (lrdPoint * fromIntegral k)

-- kËøëÈÇª
kNearestNeighbors :: (Num a) => FeatureVector a -> Int -> [FeatureVector a] -> [FeatureVector a]
kNearestNeighbors point k data_ =
  let distances = [(other, euclideanDistance point other) | other <- data_, other /= point]
      sorted = sort [(other, distance) | (other, distance) <- distances]
  in [other | (other, _) <- take k sorted]

-- Â±ÄÈÉ®ÂèØËææÂØÜÂ∫¶
localReachabilityDensity :: (Num a) => FeatureVector a -> Int -> [FeatureVector a] -> Double
localReachabilityDensity point k data_ =
  let neighbors = kNearestNeighbors point k data_
      reachabilityDistances = [reachabilityDistance point neighbor data_ | neighbor <- neighbors]
  in fromIntegral k / sum reachabilityDistances

-- ÂèØËææË∑ùÁ¶ª
reachabilityDistance :: (Num a) => 
  FeatureVector a -> 
  FeatureVector a -> 
  [FeatureVector a] -> 
  Double
reachabilityDistance point1 point2 data_ =
  let kDistance = kDistance point2 data_
      directDistance = euclideanDistance point1 point2
  in max kDistance directDistance

-- kË∑ùÁ¶ª
kDistance :: (Num a) => FeatureVector a -> [FeatureVector a] -> Double
kDistance point data_ =
  let distances = [euclideanDistance point other | other <- data_, other /= point]
      sorted = sort distances
  in sorted !! (min 4 (length sorted - 1))  -- ‰ΩøÁî®k=5
```

## üìä ËØÑ‰º∞ÊåáÊ†á

### 1. ÂàÜÁ±ªËØÑ‰º∞

```haskell
module DataMining.Evaluation where

import DataMining.Classification
import Data.List (zipWith)

-- Ê∑∑Ê∑ÜÁü©Èòµ
data ConfusionMatrix = ConfusionMatrix
  { truePositives :: Int
  , falsePositives :: Int
  , trueNegatives :: Int
  , falseNegatives :: Int
  } deriving (Show)

-- ËÆ°ÁÆóÊ∑∑Ê∑ÜÁü©Èòµ
confusionMatrix :: (Eq b) => [b] -> [b] -> ConfusionMatrix
confusionMatrix actual predicted =
  let pairs = zip actual predicted
      tp = length [() | (a, p) <- pairs, a == p, a == True]  -- ÂÅáËÆæTrue‰∏∫Ê≠£Á±ª
      fp = length [() | (a, p) <- pairs, a /= p, p == True]
      tn = length [() | (a, p) <- pairs, a == p, a == False]
      fn = length [() | (a, p) <- pairs, a /= p, p == False]
  in ConfusionMatrix tp fp tn fn

-- ÂáÜÁ°ÆÁéá
accuracy :: ConfusionMatrix -> Double
accuracy (ConfusionMatrix tp fp tn fn) =
  fromIntegral (tp + tn) / fromIntegral (tp + fp + tn + fn)

-- Á≤æÁ°ÆÁéá
precision :: ConfusionMatrix -> Double
precision (ConfusionMatrix tp fp _ _) =
  fromIntegral tp / fromIntegral (tp + fp)

-- Âè¨ÂõûÁéá
recall :: ConfusionMatrix -> Double
recall (ConfusionMatrix tp _ _ fn) =
  fromIntegral tp / fromIntegral (tp + fn)

-- F1ÂàÜÊï∞
f1Score :: ConfusionMatrix -> Double
f1Score matrix =
  let p = precision matrix
      r = recall matrix
  in 2 * p * r / (p + r)

-- ËÅöÁ±ªËØÑ‰º∞ÔºöËΩÆÂªìÁ≥ªÊï∞
silhouetteCoefficient :: (Num a) => 
  [FeatureVector a] -> 
  Clustering a -> 
  Double
silhouetteCoefficient data_ clustering =
  let coefficients = [silhouetteForPoint point clustering | point <- data_]
  in sum coefficients / fromIntegral (length coefficients)

-- Âçï‰∏™ÁÇπÁöÑËΩÆÂªìÁ≥ªÊï∞
silhouetteForPoint :: (Num a) => 
  FeatureVector a -> 
  Clustering a -> 
  Double
silhouetteForPoint point clustering =
  let cluster = findCluster point clustering
      a = averageDistance point cluster
      b = minimum [averageDistance point otherCluster | otherCluster <- clustering, 
                                                       otherCluster /= cluster]
  in (b - a) / max a b

-- ÊâæÂà∞ÁÇπÊâÄÂ±ûÁöÑËÅöÁ±ª
findCluster :: (Eq a) => FeatureVector a -> Clustering a -> Cluster a
findCluster point clustering =
  head [cluster | cluster <- clustering, point `elem` cluster]

-- Âπ≥ÂùáË∑ùÁ¶ª
averageDistance :: (Num a) => FeatureVector a -> Cluster a -> Double
averageDistance point cluster =
  let distances = [euclideanDistance point other | other <- cluster, other /= point]
  in if null distances then 0 else sum distances / fromIntegral (length distances)
```

## üìö ÂΩ¢ÂºèÂåñËØÅÊòé

### ÂÆöÁêÜ 1.1: K-meansÊî∂ÊïõÊÄß

**ÂÆöÁêÜ** K-meansÁÆóÊ≥ïÂú®ÊúâÈôêÊ≠•ÂÜÖÊî∂ÊïõÂà∞Â±ÄÈÉ®ÊúÄ‰ºòËß£„ÄÇ

**ËØÅÊòé**Ôºö

1. ÁõÆÊ†áÂáΩÊï∞ $J$ Âú®ÊØèÊ¨°Ëø≠‰ª£ÂêéÂçïË∞ÉÈÄíÂáè
2. $J$ Êúâ‰∏ãÁïåÔºàÈùûË¥üÔºâ
3. Áî±ÂçïË∞ÉÊî∂ÊïõÂÆöÁêÜÔºåÁÆóÊ≥ïÊî∂Êïõ
4. Êî∂ÊïõÁÇπÊª°Ë∂≥Â±ÄÈÉ®ÊúÄ‰ºòÊù°‰ª∂

### ÂÆöÁêÜ 1.2: AprioriÁÆóÊ≥ïÁöÑÊ≠£Á°ÆÊÄß

**ÂÆöÁêÜ** AprioriÁÆóÊ≥ïËÉΩÂ§üÊâæÂà∞ÊâÄÊúâÈ¢ëÁπÅÈ°πÈõÜ„ÄÇ

**ËØÅÊòé**Ôºö

1. ÂèçÂçïË∞ÉÊÄßÔºöÈ¢ëÁπÅÈ°πÈõÜÁöÑÂ≠êÈõÜ‰πüÊòØÈ¢ëÁπÅÁöÑ
2. ÂÄôÈÄâÁîüÊàêÔºöÊâÄÊúâÈ¢ëÁπÅkÈ°πÈõÜÈÉΩÂåÖÂê´Âú®ÂÄôÈÄâkÈ°πÈõÜ‰∏≠
3. Ââ™ÊûùÔºöÈùûÈ¢ëÁπÅÈ°πÈõÜË¢´Ê≠£Á°ÆÂâ™Êûù
4. ÂΩíÁ∫≥ÔºöÈÄöËøáÊï∞Â≠¶ÂΩíÁ∫≥Ê≥ïËØÅÊòéÊâÄÊúâÈ¢ëÁπÅÈ°πÈõÜÈÉΩË¢´ÊâæÂà∞

## üîó Áõ∏ÂÖ≥ÈìæÊé•

- [ÁªüËÆ°ÂàÜÊûê](./01-Statistical-Analysis.md)
- [Êï∞ÊçÆÂèØËßÜÂåñ](./03-Data-Visualization.md)
- [Êú∫Âô®Â≠¶‰π†](../03-Artificial-Intelligence/01-Machine-Learning.md)
- [Â§ßÊï∞ÊçÆÊäÄÊúØ](./04-Big-Data-Technology.md)

---

*Êú¨ÊñáÊ°£Âª∫Á´ã‰∫ÜÊï∞ÊçÆÊåñÊéòÁöÑÂÆåÊï¥ÂΩ¢ÂºèÂåñÁêÜËÆ∫‰ΩìÁ≥ªÔºåÂåÖÂê´‰∏•Ê†ºÁöÑÊï∞Â≠¶ÂÆö‰πâ„ÄÅHaskellÂÆûÁé∞ÂíåÂΩ¢ÂºèÂåñËØÅÊòé„ÄÇ*
