# å¤§æ•°æ®æŠ€æœ¯ - å½¢å¼åŒ–ç†è®ºä¸Haskellå®ç°

## ğŸ“‹ æ¦‚è¿°

å¤§æ•°æ®æŠ€æœ¯æ˜¯å¤„ç†ã€å­˜å‚¨å’Œåˆ†ææµ·é‡æ•°æ®çš„æŠ€æœ¯ä½“ç³»ã€‚æœ¬æ–‡æ¡£ä»å½¢å¼åŒ–è§’åº¦å»ºç«‹å¤§æ•°æ®æŠ€æœ¯çš„å®Œæ•´ç†è®ºä½“ç³»ï¼ŒåŒ…æ‹¬åˆ†å¸ƒå¼è®¡ç®—ã€æµå¤„ç†ã€å­˜å‚¨ç³»ç»Ÿç­‰æ ¸å¿ƒæŠ€æœ¯ã€‚

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### 1. å¤§æ•°æ®åŸºç¡€

#### å½¢å¼åŒ–å®šä¹‰

**å®šä¹‰ 1.1 (å¤§æ•°æ®)** å¤§æ•°æ®æ˜¯å…·æœ‰4Vç‰¹å¾çš„æ•°æ®é›†ï¼š

- **Volume**: æ•°æ®é‡ $|D| > V_{threshold}$
- **Velocity**: æ•°æ®ç”Ÿæˆé€Ÿåº¦ $v > v_{threshold}$
- **Variety**: æ•°æ®ç±»å‹å¤šæ ·æ€§ $\text{types}(D) > t_{threshold}$
- **Veracity**: æ•°æ®è´¨é‡è¦æ±‚ $Q(D) > q_{threshold}$

**å®šä¹‰ 1.2 (åˆ†å¸ƒå¼ç³»ç»Ÿ)** åˆ†å¸ƒå¼ç³»ç»Ÿæ˜¯ä¸€ä¸ªå…ƒç»„ $S = (N, C, M)$ï¼Œå…¶ä¸­ï¼š

- $N = \{n_1, \ldots, n_k\}$ æ˜¯èŠ‚ç‚¹é›†åˆ
- $C \subseteq N \times N$ æ˜¯é€šä¿¡å…³ç³»
- $M: N \to \mathcal{P}(D)$ æ˜¯æ•°æ®åˆ†å¸ƒæ˜ å°„

#### Haskellå®ç°

```haskell
{-# LANGUAGE GADTs, TypeFamilies, DataKinds #-}
{-# LANGUAGE FlexibleContexts, FlexibleInstances #-}

module BigData.Core where

import Data.Set (Set)
import qualified Data.Set as Set
import Data.Map (Map)
import qualified Data.Map as Map
import Data.List (nub)

-- å¤§æ•°æ®ç‰¹å¾
data BigDataCharacteristics = BigDataCharacteristics
  { volume :: Int64
  , velocity :: Double  -- æ•°æ®/ç§’
  , variety :: Int      -- æ•°æ®ç±»å‹æ•°é‡
  , veracity :: Double  -- æ•°æ®è´¨é‡åˆ†æ•° [0,1]
  } deriving (Show)

-- åˆ¤æ–­æ˜¯å¦ä¸ºå¤§æ•°æ®
isBigData :: BigDataCharacteristics -> Bool
isBigData chars =
  chars.volume > 1000000000 &&  -- 1GB
  chars.velocity > 1000 &&      -- 1000 records/sec
  chars.variety > 10 &&         -- 10+ data types
  chars.veracity > 0.8          -- 80% quality

-- èŠ‚ç‚¹
type NodeId = String
type Node = (NodeId, NodeState)

-- èŠ‚ç‚¹çŠ¶æ€
data NodeState = 
  Active | Inactive | Failed | Recovering
  deriving (Show, Eq)

-- åˆ†å¸ƒå¼ç³»ç»Ÿ
data DistributedSystem a = DistributedSystem
  { nodes :: Map NodeId Node
  , connections :: Set (NodeId, NodeId)
  , dataDistribution :: Map NodeId [a]
  } deriving (Show)

-- åˆ›å»ºåˆ†å¸ƒå¼ç³»ç»Ÿ
createDistributedSystem :: [NodeId] -> DistributedSystem a
createDistributedSystem nodeIds =
  let nodes = Map.fromList [(id, (id, Active)) | id <- nodeIds]
      connections = Set.fromList [(id1, id2) | id1 <- nodeIds, id2 <- nodeIds, id1 /= id2]
      dataDistribution = Map.fromList [(id, []) | id <- nodeIds]
  in DistributedSystem nodes connections dataDistribution

-- æ·»åŠ èŠ‚ç‚¹
addNode :: NodeId -> DistributedSystem a -> DistributedSystem a
addNode nodeId system =
  let newNodes = Map.insert nodeId (nodeId, Active) system.nodes
      newConnections = Set.insert (nodeId, nodeId) system.connections
      newDataDistribution = Map.insert nodeId [] system.dataDistribution
  in system { nodes = newNodes, connections = newConnections, dataDistribution = newDataDistribution }

-- ç§»é™¤èŠ‚ç‚¹
removeNode :: NodeId -> DistributedSystem a -> DistributedSystem a
removeNode nodeId system =
  let newNodes = Map.delete nodeId system.nodes
      newConnections = Set.filter (\(n1, n2) -> n1 /= nodeId && n2 /= nodeId) system.connections
      newDataDistribution = Map.delete nodeId system.dataDistribution
  in system { nodes = newNodes, connections = newConnections, dataDistribution = newDataDistribution }
```

### 2. åˆ†å¸ƒå¼è®¡ç®—æ¨¡å‹

#### å½¢å¼åŒ–å®šä¹‰

**å®šä¹‰ 1.3 (MapReduce)** MapReduceæ˜¯ä¸€ä¸ªè®¡ç®—æ¨¡å‹ï¼š
$$\text{MapReduce}(D) = \text{Reduce} \circ \text{Shuffle} \circ \text{Map}(D)$$

å…¶ä¸­ï¼š

- $\text{Map}: D \to \{(k_1, v_1), \ldots, (k_n, v_n)\}$
- $\text{Shuffle}: \{(k_i, v_i)\} \to \{k_j, [v_{j1}, \ldots, v_{jm}]\}$
- $\text{Reduce}: \{k_j, [v_{j1}, \ldots, v_{jm}]\} \to \{(k_j, v_j')\}$

**å®šä¹‰ 1.4 (åˆ†å¸ƒå¼å›¾è®¡ç®—)** åˆ†å¸ƒå¼å›¾è®¡ç®—æ¨¡å‹ï¼š
$$G = (V, E) \text{ where } V = \bigcup_{i=1}^n V_i, E = \bigcup_{i=1}^n E_i$$

#### Haskellå®ç°

```haskell
module BigData.DistributedComputing where

import BigData.Core
import Data.Map (Map)
import qualified Data.Map as Map
import Data.List (groupBy, sortBy)
import Data.Ord (comparing)

-- MapReduceæ¨¡å‹
data MapReduce k1 v1 k2 v2 k3 v3 = MapReduce
  { mapFunction :: v1 -> [(k2, v2)]
  , reduceFunction :: k2 -> [v2] -> [(k3, v3)]
  } deriving (Show)

-- æ‰§è¡ŒMapReduce
executeMapReduce :: (Ord k2) => 
  MapReduce k1 v1 k2 v2 k3 v3 -> 
  [v1] -> 
  [(k3, v3)]
executeMapReduce mr input =
  let -- Mapé˜¶æ®µ
      mapped = concat [mr.mapFunction value | value <- input]
      -- Shuffleé˜¶æ®µ
      grouped = groupByKey mapped
      -- Reduceé˜¶æ®µ
      reduced = concat [mr.reduceFunction key values | (key, values) <- grouped]
  in reduced

-- æŒ‰é”®åˆ†ç»„
groupByKey :: (Ord k) => [(k, v)] -> [(k, [v])]
groupByKey keyValuePairs =
  let sorted = sortBy (comparing fst) keyValuePairs
      grouped = groupBy (\a b -> fst a == fst b) sorted
  in [(fst (head group), map snd group) | group <- grouped]

-- åˆ†å¸ƒå¼å›¾
data DistributedGraph a = DistributedGraph
  { vertices :: Map String a
  , edges :: Set (String, String)
  , partitions :: Map NodeId (Set String)  -- èŠ‚ç‚¹åˆ°é¡¶ç‚¹çš„æ˜ å°„
  } deriving (Show)

-- åˆ›å»ºåˆ†å¸ƒå¼å›¾
createDistributedGraph :: [String] -> [(String, String)] -> [NodeId] -> DistributedGraph a
createDistributedGraph vertexIds edgePairs nodeIds =
  let vertices = Map.fromList [(id, undefined) | id <- vertexIds]  -- ç®€åŒ–å®ç°
      edges = Set.fromList edgePairs
      numVertices = length vertexIds
      numNodes = length nodeIds
      verticesPerNode = numVertices `div` numNodes
      partitions = Map.fromList [(nodeIds !! i, 
                                 Set.fromList (take verticesPerNode (drop (i * verticesPerNode) vertexIds))) |
                                i <- [0..numNodes-1]]
  in DistributedGraph vertices edges partitions

-- å›¾ç®—æ³•ï¼šPageRank
data PageRank = PageRank
  { nodeId :: String
  , rank :: Double
  } deriving (Show)

-- è®¡ç®—PageRank
pageRank :: DistributedGraph a -> Double -> Int -> Map String Double
pageRank graph dampingFactor iterations =
  let initialRanks = Map.fromList [(nodeId, 1.0) | nodeId <- Map.keys graph.vertices]
  in iterate (updatePageRank graph dampingFactor) initialRanks !! iterations

-- æ›´æ–°PageRank
updatePageRank :: DistributedGraph a -> Double -> Map String Double -> Map String Double
updatePageRank graph dampingFactor currentRanks =
  let numVertices = fromIntegral (Map.size graph.vertices)
      baseRank = (1 - dampingFactor) / numVertices
      
      -- è®¡ç®—æ¯ä¸ªé¡¶ç‚¹çš„å‡ºåº¦
      outDegrees = Map.fromListWith (+) [(from, 1) | (from, to) <- Set.toList graph.edges]
      
      -- æ›´æ–°æ¯ä¸ªé¡¶ç‚¹çš„PageRank
      newRanks = Map.mapWithKey (\nodeId currentRank ->
        let incomingEdges = [(from, to) | (from, to) <- Set.toList graph.edges, to == nodeId]
            incomingRank = sum [currentRanks Map.! from / (outDegrees Map.! from) | 
                              (from, _) <- incomingEdges]
        in baseRank + dampingFactor * incomingRank) currentRanks
  in newRanks

-- åˆ†å¸ƒå¼æµå¤„ç†
data StreamProcessor a b = StreamProcessor
  { processFunction :: a -> [b]
  , windowSize :: Int
  , slidingStep :: Int
  } deriving (Show)

-- æ»‘åŠ¨çª—å£
data SlidingWindow a = SlidingWindow
  { window :: [a]
  , maxSize :: Int
  } deriving (Show)

-- åˆ›å»ºæ»‘åŠ¨çª—å£
createSlidingWindow :: Int -> SlidingWindow a
createSlidingWindow size = SlidingWindow [] size

-- æ·»åŠ å…ƒç´ åˆ°çª—å£
addToWindow :: a -> SlidingWindow a -> SlidingWindow a
addToWindow element window =
  let newWindow = element : window.window
      trimmedWindow = take window.maxSize newWindow
  in window { window = trimmedWindow }

-- å¤„ç†æ»‘åŠ¨çª—å£
processWindow :: (a -> b) -> SlidingWindow a -> [b]
processWindow f window = map f window.window
```

### 3. å­˜å‚¨ç³»ç»Ÿ

#### å½¢å¼åŒ–å®šä¹‰

**å®šä¹‰ 1.5 (åˆ†å¸ƒå¼å­˜å‚¨)** åˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿæ˜¯ä¸€ä¸ªå…ƒç»„ $S = (N, R, D)$ï¼Œå…¶ä¸­ï¼š

- $N$ æ˜¯å­˜å‚¨èŠ‚ç‚¹é›†åˆ
- $R: D \to \mathcal{P}(N)$ æ˜¯å¤åˆ¶ç­–ç•¥
- $D$ æ˜¯æ•°æ®é›†åˆ

**å®šä¹‰ 1.6 (ä¸€è‡´æ€§å“ˆå¸Œ)** ä¸€è‡´æ€§å“ˆå¸Œå°†æ•°æ®æ˜ å°„åˆ°èŠ‚ç‚¹ï¼š
$$h: D \to N \text{ where } h(d) = \arg\min_{n \in N} \text{dist}(h(d), h(n))$$

#### Haskellå®ç°

```haskell
module BigData.Storage where

import BigData.Core
import Data.Map (Map)
import qualified Data.Map as Map
import Data.List (sortBy)
import Data.Ord (comparing)

-- å­˜å‚¨èŠ‚ç‚¹
data StorageNode = StorageNode
  { nodeId :: String
  , capacity :: Int64
  , usedSpace :: Int64
  , dataItems :: Map String DataItem
  } deriving (Show)

-- æ•°æ®é¡¹
data DataItem = DataItem
  { key :: String
  , value :: String
  , size :: Int64
  , replicationFactor :: Int
  } deriving (Show)

-- åˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿ
data DistributedStorage = DistributedStorage
  { nodes :: Map String StorageNode
  , replicationStrategy :: ReplicationStrategy
  , hashFunction :: String -> Int
  } deriving (Show)

-- å¤åˆ¶ç­–ç•¥
data ReplicationStrategy = 
  SimpleReplication Int |  -- ç®€å•å¤åˆ¶
  ConsistentHashing |     -- ä¸€è‡´æ€§å“ˆå¸Œ
  RaftReplication         -- Raftåè®®
  deriving (Show)

-- åˆ›å»ºåˆ†å¸ƒå¼å­˜å‚¨
createDistributedStorage :: [String] -> ReplicationStrategy -> DistributedStorage
createDistributedStorage nodeIds strategy =
  let nodes = Map.fromList [(id, StorageNode id 1000000000 0 Map.empty) | id <- nodeIds]
      hashFunction = \key -> hash key `mod` 1000000
  in DistributedStorage nodes strategy hashFunction

-- ç®€å•å“ˆå¸Œå‡½æ•°
hash :: String -> Int
hash = foldl (\acc char -> acc * 31 + fromEnum char) 0

-- å­˜å‚¨æ•°æ®
storeData :: DistributedStorage -> String -> String -> DistributedStorage
storeData storage key value =
  case storage.replicationStrategy of
    SimpleReplication factor -> simpleReplication storage key value factor
    ConsistentHashing -> consistentHashingReplication storage key value
    RaftReplication -> raftReplication storage key value

-- ç®€å•å¤åˆ¶
simpleReplication :: DistributedStorage -> String -> String -> Int -> DistributedStorage
simpleReplication storage key value factor =
  let dataItem = DataItem key value (fromIntegral (length value)) factor
      nodeIds = Map.keys storage.nodes
      selectedNodes = take factor nodeIds
      updatedNodes = foldl (\nodes nodeId ->
        let node = nodes Map.! nodeId
            updatedNode = node { 
              usedSpace = node.usedSpace + dataItem.size,
              dataItems = Map.insert key dataItem node.dataItems
            }
        in Map.insert nodeId updatedNode nodes) storage.nodes selectedNodes
  in storage { nodes = updatedNodes }

-- ä¸€è‡´æ€§å“ˆå¸Œå¤åˆ¶
consistentHashingReplication :: DistributedStorage -> String -> String -> DistributedStorage
consistentHashingReplication storage key value =
  let dataItem = DataItem key value (fromIntegral (length value)) 3
      keyHash = storage.hashFunction key
      nodeHashes = [(nodeId, storage.hashFunction nodeId) | nodeId <- Map.keys storage.nodes]
      sortedNodes = sortBy (comparing snd) nodeHashes
      selectedNodes = take 3 [nodeId | (nodeId, _) <- sortedNodes, 
                                     storage.hashFunction nodeId >= keyHash]
      updatedNodes = foldl (\nodes nodeId ->
        let node = nodes Map.! nodeId
            updatedNode = node { 
              usedSpace = node.usedSpace + dataItem.size,
              dataItems = Map.insert key dataItem node.dataItems
            }
        in Map.insert nodeId updatedNode nodes) storage.nodes selectedNodes
  in storage { nodes = updatedNodes }

-- Raftå¤åˆ¶ï¼ˆç®€åŒ–å®ç°ï¼‰
raftReplication :: DistributedStorage -> String -> String -> DistributedStorage
raftReplication storage key value =
  -- ç®€åŒ–å®ç°ï¼šé€‰æ‹©ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ä½œä¸ºleader
  let leaderId = head (Map.keys storage.nodes)
      dataItem = DataItem key value (fromIntegral (length value)) 3
      leader = storage.nodes Map.! leaderId
      updatedLeader = leader { 
        usedSpace = leader.usedSpace + dataItem.size,
        dataItems = Map.insert key dataItem leader.dataItems
      }
      updatedNodes = Map.insert leaderId updatedLeader storage.nodes
  in storage { nodes = updatedNodes }

-- æ£€ç´¢æ•°æ®
retrieveData :: DistributedStorage -> String -> Maybe String
retrieveData storage key =
  let nodeIds = Map.keys storage.nodes
      nodeWithData = find (\nodeId -> 
        let node = storage.nodes Map.! nodeId
        in Map.member key node.dataItems) nodeIds
  in case nodeWithData of
       Just nodeId -> Just (value (storage.nodes Map.! nodeId).dataItems Map.! key)
       Nothing -> Nothing

-- æŸ¥æ‰¾åŒ…å«æ•°æ®çš„èŠ‚ç‚¹
find :: (a -> Bool) -> [a] -> Maybe a
find _ [] = Nothing
find p (x:xs) = if p x then Just x else find p xs
```

### 4. æµå¤„ç†ç³»ç»Ÿ

#### å½¢å¼åŒ–å®šä¹‰

**å®šä¹‰ 1.7 (æ•°æ®æµ)** æ•°æ®æµæ˜¯ä¸€ä¸ªæ— é™åºåˆ—ï¼š
$$S = \langle s_1, s_2, s_3, \ldots \rangle$$

**å®šä¹‰ 1.8 (æµå¤„ç†ç®—å­)** æµå¤„ç†ç®—å­ $F$ å°†è¾“å…¥æµè½¬æ¢ä¸ºè¾“å‡ºæµï¼š
$$F: S_{in} \to S_{out}$$

#### Haskellå®ç°

```haskell
module BigData.StreamProcessing where

import BigData.Core
import Data.List (scanl, filter)
import Control.Concurrent (forkIO, threadDelay)
import Control.Monad (forever)

-- æ•°æ®æµ
type Stream a = [a]

-- æµå¤„ç†ç®—å­
type StreamOperator a b = Stream a -> Stream b

-- åŸºç¡€æµç®—å­
mapStream :: (a -> b) -> StreamOperator a b
mapStream f = map f

filterStream :: (a -> Bool) -> StreamOperator a a
filterStream p = filter p

reduceStream :: (b -> a -> b) -> b -> StreamOperator a b
reduceStream f initial = scanl f initial

-- çª—å£ç®—å­
data Window a = Window
  { elements :: [a]
  , size :: Int
  } deriving (Show)

-- æ»‘åŠ¨çª—å£
slidingWindow :: Int -> StreamOperator a (Window a)
slidingWindow windowSize stream =
  let windows = [Window (take windowSize (drop i stream)) windowSize | 
                 i <- [0..length stream - windowSize]]
  in windows

-- æ»šåŠ¨çª—å£
tumblingWindow :: Int -> StreamOperator a (Window a)
tumblingWindow windowSize stream =
  let chunks = [take windowSize (drop (i * windowSize) stream) | 
                i <- [0..(length stream - 1) `div` windowSize]]
      windows = [Window chunk windowSize | chunk <- chunks, length chunk == windowSize]
  in windows

-- æµå¤„ç†ä½œä¸š
data StreamJob a b = StreamJob
  { inputStream :: Stream a
  , operators :: [StreamOperator a b]
  , outputStream :: Stream b
  } deriving (Show)

-- åˆ›å»ºæµå¤„ç†ä½œä¸š
createStreamJob :: Stream a -> [StreamOperator a b] -> StreamJob a b
createStreamJob input operators =
  let output = foldl (\stream op -> op stream) input operators
  in StreamJob input operators output

-- å®æ—¶æµå¤„ç†
data RealTimeStream a = RealTimeStream
  { source :: IO a
  , processors :: [a -> b]
  , sink :: b -> IO ()
  } deriving (Show)

-- è¿è¡Œå®æ—¶æµå¤„ç†
runRealTimeStream :: RealTimeStream a -> IO ()
runRealTimeStream stream = forever $ do
  data_ <- stream.source
  let processed = foldl (\acc processor -> processor acc) data_ stream.processors
  stream.sink processed

-- ç¤ºä¾‹ï¼šå•è¯è®¡æ•°æµå¤„ç†
wordCountStream :: Stream String -> Stream (String, Int)
wordCountStream textStream =
  let words = concatMap words textStream
      wordPairs = [(word, 1) | word <- words]
      grouped = groupByKey wordPairs
      counted = [(word, sum counts) | (word, counts) <- grouped]
  in counted

-- æŒ‰é”®åˆ†ç»„ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰
groupByKey :: (Eq k) => [(k, v)] -> [(k, [v])]
groupByKey [] = []
groupByKey ((k, v):rest) =
  let (same, different) = partition (\(k', _) -> k' == k) rest
      group = (k, v : map snd same)
  in group : groupByKey different

-- åˆ†åŒºå‡½æ•°
partition :: (a -> Bool) -> [a] -> ([a], [a])
partition p = foldr (\x (yes, no) -> if p x then (x:yes, no) else (yes, x:no)) ([], [])

-- åˆ†å¸ƒå¼æµå¤„ç†
data DistributedStreamProcessor a b = DistributedStreamProcessor
  { partitions :: Int
  , partitionFunction :: a -> Int
  , localProcessor :: StreamOperator a b
  , mergeFunction :: [b] -> b
  } deriving (Show)

-- æ‰§è¡Œåˆ†å¸ƒå¼æµå¤„ç†
executeDistributedStream :: (Eq b) => 
  DistributedStreamProcessor a b -> 
  Stream a -> 
  Stream b
executeDistributedStream processor stream =
  let -- åˆ†åŒº
      partitioned = partitionStream processor.partitions processor.partitionFunction stream
      -- æœ¬åœ°å¤„ç†
      processed = map processor.localProcessor partitioned
      -- åˆå¹¶ç»“æœ
      merged = mergeStreams processor.mergeFunction processed
  in merged

-- æµåˆ†åŒº
partitionStream :: Int -> (a -> Int) -> Stream a -> [Stream a]
partitionStream numPartitions partitionFunc stream =
  let partitions = replicate numPartitions []
      addToPartition partitions element =
        let partitionIndex = partitionFunc element `mod` numPartitions
            currentPartition = partitions !! partitionIndex
            newPartition = element : currentPartition
        in take partitionIndex partitions ++ [newPartition] ++ drop (partitionIndex + 1) partitions
  in foldl addToPartition partitions stream

-- åˆå¹¶æµ
mergeStreams :: ([b] -> b) -> [Stream b] -> Stream b
mergeStreams mergeFunc streams =
  let interleaved = concat (transpose streams)
      merged = [mergeFunc chunk | chunk <- chunksOf (length streams) interleaved]
  in merged

-- åˆ†å—
chunksOf :: Int -> [a] -> [[a]]
chunksOf _ [] = []
chunksOf n xs = take n xs : chunksOf n (drop n xs)

-- è½¬ç½®
transpose :: [[a]] -> [[a]]
transpose [] = []
transpose ([]:_) = []
transpose xs = (map head xs) : transpose (map tail xs)
```

## ğŸ”¬ é«˜çº§å¤§æ•°æ®æŠ€æœ¯

### 1. æœºå™¨å­¦ä¹ é›†æˆ

#### å½¢å¼åŒ–å®šä¹‰

**å®šä¹‰ 1.9 (åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ )** åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ æ¨¡å‹ï¼š
$$\min_w \sum_{i=1}^n L(w, x_i, y_i) + \lambda R(w)$$

å…¶ä¸­æŸå¤±å‡½æ•° $L$ åœ¨åˆ†å¸ƒå¼èŠ‚ç‚¹ä¸Šè®¡ç®—ã€‚

#### Haskellå®ç°

```haskell
module BigData.MachineLearning where

import BigData.Core
import Data.List (zipWith, sum)
import Data.Map (Map)
import qualified Data.Map as Map

-- æœºå™¨å­¦ä¹ æ¨¡å‹
data MLModel a b = MLModel
  { parameters :: Map String Double
  , lossFunction :: (Map String Double, a, b) -> Double
  , gradientFunction :: (Map String Double, a, b) -> Map String Double
  , regularization :: Map String Double -> Double
  } deriving (Show)

-- åˆ†å¸ƒå¼æ¢¯åº¦ä¸‹é™
data DistributedGradientDescent = DistributedGradientDescent
  { learningRate :: Double
  , batchSize :: Int
  , maxIterations :: Int
  } deriving (Show)

-- æ‰§è¡Œåˆ†å¸ƒå¼æ¢¯åº¦ä¸‹é™
distributedGradientDescent :: 
  MLModel a b -> 
  DistributedGradientDescent -> 
  [(a, b)] -> 
  Map String Double
distributedGradientDescent model config data_ =
  let initialParams = model.parameters
      iterations = [0..config.maxIterations-1]
      updatedParams = foldl (\params iteration ->
        let batch = take config.batchSize (drop (iteration * config.batchSize) data_)
            gradients = [model.gradientFunction (params, x, y) | (x, y) <- batch]
            avgGradient = averageGradients gradients
            newParams = updateParameters params avgGradient config.learningRate
        in newParams) initialParams iterations
  in updatedParams

-- å¹³å‡æ¢¯åº¦
averageGradients :: [Map String Double] -> Map String Double
averageGradients gradients =
  let allKeys = nub (concatMap Map.keys gradients)
      avgGradient = Map.fromList [(key, sum [grad Map.! key | grad <- gradients]) / fromIntegral (length gradients) |
                                 key <- allKeys]
  in avgGradient

-- æ›´æ–°å‚æ•°
updateParameters :: Map String Double -> Map String Double -> Double -> Map String Double
updateParameters params gradients learningRate =
  Map.mapWithKey (\key param ->
    param - learningRate * (gradients Map.! key)) params

-- å»é‡
nub :: (Eq a) => [a] -> [a]
nub [] = []
nub (x:xs) = x : nub (filter (/= x) xs)

-- çº¿æ€§å›å½’æ¨¡å‹
linearRegressionModel :: MLModel [Double] Double
linearRegressionModel = MLModel
  { parameters = Map.fromList [("w0", 0.0), ("w1", 0.0)]
  , lossFunction = \(params, x, y) ->
      let prediction = (params Map.! "w0") + sum (zipWith (*) x (repeat (params Map.! "w1")))
      in (prediction - y)^2 / 2
  , gradientFunction = \(params, x, y) ->
      let prediction = (params Map.! "w0") + sum (zipWith (*) x (repeat (params Map.! "w1")))
          error = prediction - y
      in Map.fromList [("w0", error), ("w1", error * sum x)]
  , regularization = \params ->
      let w0 = params Map.! "w0"
          w1 = params Map.! "w1"
      in 0.01 * (w0^2 + w1^2) / 2
  }
```

### 2. å®æ—¶åˆ†æ

#### å½¢å¼åŒ–å®šä¹‰

**å®šä¹‰ 1.10 (å®æ—¶åˆ†æ)** å®æ—¶åˆ†æåœ¨æ—¶é—´çª—å£ $T$ å†…å¤„ç†æ•°æ®ï¼š
$$A_T(D) = \text{analyze}(\{d \in D : \text{time}(d) \in T\})$$

#### Haskellå®ç°

```haskell
module BigData.RealTimeAnalytics where

import BigData.Core
import BigData.StreamProcessing
import Data.Time (UTCTime, getCurrentTime, diffUTCTime)
import Control.Concurrent (forkIO, threadDelay)

-- å®æ—¶åˆ†æçª—å£
data TimeWindow = TimeWindow
  { startTime :: UTCTime
  , endTime :: UTCTime
  , data_ :: [TimeStampedData]
  } deriving (Show)

-- æ—¶é—´æˆ³æ•°æ®
data TimeStampedData = TimeStampedData
  { timestamp :: UTCTime
  , value :: Double
  } deriving (Show)

-- å®æ—¶åˆ†æå™¨
data RealTimeAnalyzer a b = RealTimeAnalyzer
  { windowSize :: Int  -- çª—å£å¤§å°ï¼ˆç§’ï¼‰
  , analysisFunction :: [a] -> b
  , outputHandler :: b -> IO ()
  } deriving (Show)

-- è¿è¡Œå®æ—¶åˆ†æ
runRealTimeAnalysis :: RealTimeAnalyzer TimeStampedData Double -> IO ()
runRealTimeAnalysis analyzer = do
  let window = []
  forever $ do
    currentTime <- getCurrentTime
    -- æ¨¡æ‹Ÿæ¥æ”¶æ•°æ®
    newData <- generateTimeStampedData currentTime
    let updatedWindow = addToTimeWindow window newData analyzer.windowSize
        analysisResult = analyzer.analysisFunction (map value updatedWindow)
    analyzer.outputHandler analysisResult
    threadDelay 1000000  -- 1ç§’

-- ç”Ÿæˆæ—¶é—´æˆ³æ•°æ®ï¼ˆæ¨¡æ‹Ÿï¼‰
generateTimeStampedData :: UTCTime -> IO TimeStampedData
generateTimeStampedData time = do
  -- æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆ
  return $ TimeStampedData time (fromIntegral (round time) `mod` 100)

-- æ·»åŠ åˆ°æ—¶é—´çª—å£
addToTimeWindow :: [TimeStampedData] -> TimeStampedData -> Int -> [TimeStampedData]
addToTimeWindow window newData windowSize =
  let currentTime = timestamp newData
      cutoffTime = addUTCTime (fromIntegral (-windowSize)) currentTime
      filteredWindow = filter (\data_ -> timestamp data_ > cutoffTime) window
  in newData : filteredWindow

-- æ·»åŠ æ—¶é—´ï¼ˆç®€åŒ–å®ç°ï¼‰
addUTCTime :: Double -> UTCTime -> UTCTime
addUTCTime seconds time = time  -- ç®€åŒ–å®ç°

-- å®æ—¶èšåˆ
data RealTimeAggregation = RealTimeAggregation
  { count :: Int
  , sum :: Double
  , average :: Double
  , min :: Double
  , max :: Double
  } deriving (Show)

-- è®¡ç®—å®æ—¶èšåˆ
computeRealTimeAggregation :: [Double] -> RealTimeAggregation
computeRealTimeAggregation values =
  let count = length values
      sum_ = Data.List.sum values
      average = if count > 0 then sum_ / fromIntegral count else 0
      min_ = minimum values
      max_ = maximum values
  in RealTimeAggregation count sum_ average min_ max_

-- å¼‚å¸¸æ£€æµ‹
data AnomalyDetector = AnomalyDetector
  { threshold :: Double
  , baseline :: Double
  , sensitivity :: Double
  } deriving (Show)

-- æ£€æµ‹å¼‚å¸¸
detectAnomaly :: AnomalyDetector -> Double -> Bool
detectAnomaly detector value =
  let deviation = abs (value - detector.baseline)
      normalizedDeviation = deviation / detector.baseline
  in normalizedDeviation > detector.threshold * detector.sensitivity

-- å®æ—¶ç›‘æ§ç³»ç»Ÿ
data RealTimeMonitor a = RealTimeMonitor
  { dataSource :: IO a
  , analyzers :: [a -> Double]
  , detectors :: [AnomalyDetector]
  , alertHandler :: String -> IO ()
  } deriving (Show)

-- è¿è¡Œå®æ—¶ç›‘æ§
runRealTimeMonitor :: RealTimeMonitor TimeStampedData -> IO ()
runRealTimeMonitor monitor = forever $ do
  data_ <- monitor.dataSource
  let values = [analyzer data_ | analyzer <- monitor.analyzers]
      anomalies = zipWith detectAnomaly monitor.detectors values
      anomalyIndices = [i | (i, isAnomaly) <- zip [0..] anomalies, isAnomaly]
  
  if not (null anomalyIndices)
    then monitor.alertHandler ("Anomaly detected in metrics: " ++ show anomalyIndices)
    else return ()
  
  threadDelay 1000000  -- 1ç§’
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### 1. ç¼“å­˜ç­–ç•¥

```haskell
module BigData.PerformanceOptimization where

import BigData.Core
import Data.Map (Map)
import qualified Data.Map as Map
import Data.List (sortBy)
import Data.Ord (comparing)

-- ç¼“å­˜ç­–ç•¥
data CachePolicy = 
  LRU |  -- æœ€è¿‘æœ€å°‘ä½¿ç”¨
  LFU |  -- æœ€å°‘ä½¿ç”¨
  FIFO   -- å…ˆè¿›å…ˆå‡º
  deriving (Show)

-- ç¼“å­˜é¡¹
data CacheItem a = CacheItem
  { key :: String
  , value :: a
  , accessCount :: Int
  , lastAccess :: Int  -- æ—¶é—´æˆ³
  } deriving (Show)

-- ç¼“å­˜
data Cache a = Cache
  { items :: Map String (CacheItem a)
  , maxSize :: Int
  , policy :: CachePolicy
  , currentTime :: Int
  } deriving (Show)

-- åˆ›å»ºç¼“å­˜
createCache :: Int -> CachePolicy -> Cache a
createCache maxSize policy = Cache Map.empty maxSize policy 0

-- è·å–ç¼“å­˜é¡¹
getFromCache :: (Eq a) => String -> Cache a -> (Maybe a, Cache a)
getFromCache key cache =
  case Map.lookup key cache.items of
    Just item -> 
      let updatedItem = item { 
        accessCount = item.accessCount + 1,
        lastAccess = cache.currentTime
      }
          updatedCache = cache { 
        items = Map.insert key updatedItem cache.items,
        currentTime = cache.currentTime + 1
      }
      in (Just item.value, updatedCache)
    Nothing -> (Nothing, cache { currentTime = cache.currentTime + 1 })

-- æ·»åŠ åˆ°ç¼“å­˜
addToCache :: String -> a -> Cache a -> Cache a
addToCache key value cache =
  let newItem = CacheItem key value 1 cache.currentTime
      updatedItems = Map.insert key newItem cache.items
      cacheWithNewItem = cache { 
        items = updatedItems,
        currentTime = cache.currentTime + 1
      }
  in if Map.size updatedItems > cache.maxSize
     then evictItem cacheWithNewItem
     else cacheWithNewItem

-- é©±é€ç¼“å­˜é¡¹
evictItem :: Cache a -> Cache a
evictItem cache =
  case cache.policy of
    LRU -> evictLRU cache
    LFU -> evictLFU cache
    FIFO -> evictFIFO cache

-- é©±é€æœ€è¿‘æœ€å°‘ä½¿ç”¨çš„é¡¹
evictLRU :: Cache a -> Cache a
evictLRU cache =
  let items = Map.toList cache.items
      lruItem = minimumBy (comparing (lastAccess . snd)) items
      keyToEvict = fst lruItem
  in cache { items = Map.delete keyToEvict cache.items }

-- é©±é€æœ€å°‘ä½¿ç”¨çš„é¡¹
evictLFU :: Cache a -> Cache a
evictLFU cache =
  let items = Map.toList cache.items
      lfuItem = minimumBy (comparing (accessCount . snd)) items
      keyToEvict = fst lfuItem
  in cache { items = Map.delete keyToEvict cache.items }

-- é©±é€å…ˆè¿›å…ˆå‡ºçš„é¡¹
evictFIFO :: Cache a -> Cache a
evictFIFO cache =
  let items = Map.toList cache.items
      fifoItem = minimumBy (comparing (lastAccess . snd)) items
      keyToEvict = fst fifoItem
  in cache { items = Map.delete keyToEvict cache.items }

-- æ€§èƒ½ç›‘æ§
data PerformanceMetrics = PerformanceMetrics
  { throughput :: Double  -- å¤„ç†é€Ÿåº¦
  , latency :: Double     -- å»¶è¿Ÿ
  , errorRate :: Double   -- é”™è¯¯ç‡
  , resourceUsage :: Double  -- èµ„æºä½¿ç”¨ç‡
  } deriving (Show)

-- è®¡ç®—æ€§èƒ½æŒ‡æ ‡
calculatePerformanceMetrics :: [Double] -> [Double] -> [Bool] -> Double -> PerformanceMetrics
calculatePerformanceMetrics throughputs latencies errors resourceUsage =
  let avgThroughput = sum throughputs / fromIntegral (length throughputs)
      avgLatency = sum latencies / fromIntegral (length latencies)
      errorRate = fromIntegral (length (filter id errors)) / fromIntegral (length errors)
  in PerformanceMetrics avgThroughput avgLatency errorRate resourceUsage
```

## ğŸ“š å½¢å¼åŒ–è¯æ˜

### å®šç† 1.1: MapReduceçš„æ­£ç¡®æ€§

**å®šç†** MapReduceç®—æ³•èƒ½å¤Ÿæ­£ç¡®è®¡ç®—åˆ†å¸ƒå¼æ•°æ®ä¸Šçš„èšåˆå‡½æ•°ã€‚

**è¯æ˜**ï¼š

1. Mapé˜¶æ®µï¼šæ¯ä¸ªæ•°æ®é¡¹è¢«ç‹¬ç«‹å¤„ç†ï¼Œç”Ÿæˆé”®å€¼å¯¹
2. Shuffleé˜¶æ®µï¼šç›¸åŒé”®çš„å€¼è¢«åˆ†ç»„ï¼Œä¿æŒæ•°æ®å®Œæ•´æ€§
3. Reduceé˜¶æ®µï¼šæ¯ä¸ªé”®çš„å€¼è¢«èšåˆï¼Œå¾—åˆ°æœ€ç»ˆç»“æœ
4. ç”±å‡½æ•°çš„ç»“åˆå¾‹ï¼Œåˆ†å¸ƒå¼è®¡ç®—ç­‰ä»·äºé›†ä¸­å¼è®¡ç®—

### å®šç† 1.2: ä¸€è‡´æ€§å“ˆå¸Œçš„è´Ÿè½½å‡è¡¡

**å®šç†** ä¸€è‡´æ€§å“ˆå¸Œåœ¨èŠ‚ç‚¹å˜åŒ–æ—¶æœ€å°åŒ–æ•°æ®é‡åˆ†å¸ƒã€‚

**è¯æ˜**ï¼š

1. å½“èŠ‚ç‚¹ $n$ åŠ å…¥æ—¶ï¼Œåªæœ‰ $n$ çš„åç»§èŠ‚ç‚¹éœ€è¦é‡åˆ†å¸ƒæ•°æ®
2. å½“èŠ‚ç‚¹ $n$ ç¦»å¼€æ—¶ï¼Œåªæœ‰ $n$ çš„æ•°æ®éœ€è¦é‡åˆ†å¸ƒ
3. é‡åˆ†å¸ƒçš„æ•°æ®é‡çº¦ä¸º $\frac{|D|}{|N|}$ï¼Œå…¶ä¸­ $|D|$ æ˜¯æ•°æ®é‡ï¼Œ$|N|$ æ˜¯èŠ‚ç‚¹æ•°
4. å› æ­¤è´Ÿè½½å‡è¡¡æ€§å¾—åˆ°ä¿è¯

## ğŸ”— ç›¸å…³é“¾æ¥

- [ç»Ÿè®¡åˆ†æ](./01-Statistical-Analysis.md)
- [æ•°æ®æŒ–æ˜](./02-Data-Mining.md)
- [æ•°æ®å¯è§†åŒ–](./03-Data-Visualization.md)
- [åˆ†å¸ƒå¼ç³»ç»Ÿç†è®º](../03-Theory/03-Distributed-System-Theory.md)

---

*æœ¬æ–‡æ¡£å»ºç«‹äº†å¤§æ•°æ®æŠ€æœ¯çš„å®Œæ•´å½¢å¼åŒ–ç†è®ºä½“ç³»ï¼ŒåŒ…å«ä¸¥æ ¼çš„æ•°å­¦å®šä¹‰ã€Haskellå®ç°å’Œå½¢å¼åŒ–è¯æ˜ã€‚*
