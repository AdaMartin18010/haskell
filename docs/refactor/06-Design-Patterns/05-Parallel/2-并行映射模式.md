# 并行映射模式 (Parallel Map Pattern)

## 概述

并行映射模式是一种并行设计模式，它将一个函数并行地应用到集合的每个元素上。这种模式充分利用多核处理器的能力，通过将工作负载分配到多个线程或进程来提高性能。

## 设计原则

- **数据并行**: 将数据分割到多个处理单元
- **负载均衡**: 确保各处理单元的工作量均衡
- **容错性**: 处理并行执行中的错误
- **可扩展性**: 支持动态调整并行度

## 模式结构

```text
Input Data
  ↓
Split (数据分割)
  ↓
Parallel Workers (并行工作器)
  ↓
Map Function (映射函数)
  ↓
Combine (结果合并)
```

## Haskell实现

### 基础并行映射

```haskell
import Control.Parallel
import Control.Parallel.Strategies
import Data.List (splitAt)

-- 基础并行映射
parMap :: (a -> b) -> [a] -> [b]
parMap f xs = map f xs `using` parList rseq

-- 分块并行映射
chunkedParMap :: Int -> (a -> b) -> [a] -> [b]
chunkedParMap chunkSize f xs = 
  let chunks = splitIntoChunks chunkSize xs
      mappedChunks = map (map f) chunks `using` parList rseq
  in concat mappedChunks

-- 分割列表为块
splitIntoChunks :: Int -> [a] -> [[a]]
splitIntoChunks _ [] = []
splitIntoChunks n xs = 
  let (chunk, rest) = splitAt n xs
  in chunk : splitIntoChunks n rest

-- 使用示例
main :: IO ()
main = do
  let numbers = [1..1000000]
  let result = parMap (*2) numbers
  putStrLn $ "Parallel map result: " ++ show (take 10 result)
```

### 使用Par Monad的并行映射

```haskell
import Control.Monad.Par
import Control.Monad.Par.Combinator

-- Par Monad并行映射
parMapPar :: NFData b => (a -> b) -> [a] -> Par [b]
parMapPar f xs = do
  vars <- mapM spawn (map f xs)
  mapM get vars

-- 分块Par Monad映射
chunkedParMapPar :: NFData b => Int -> (a -> b) -> [a] -> Par [b]
chunkedParMapPar chunkSize f xs = do
  let chunks = splitIntoChunks chunkSize xs
  chunkResults <- mapM (parMapPar f) chunks
  return $ concat chunkResults

-- 使用示例
demoPar :: IO ()
demoPar = do
  let numbers = [1..100000]
  result <- runPar $ parMapPar (*2) numbers
  putStrLn $ "Par monad result: " ++ show (take 10 result)
```

### 使用Async的并行映射

```haskell
import Control.Concurrent.Async
import Control.Monad

-- Async并行映射
asyncParMap :: (a -> b) -> [a] -> IO [b]
asyncParMap f xs = do
  asyncs <- mapM (async . return . f) xs
  mapM wait asyncs

-- 分块Async映射
chunkedAsyncParMap :: Int -> (a -> b) -> [a] -> IO [b]
chunkedAsyncParMap chunkSize f xs = do
  let chunks = splitIntoChunks chunkSize xs
  chunkAsyncs <- mapM (async . map f) chunks
  chunkResults <- mapM wait chunkAsyncs
  return $ concat chunkResults

-- 带错误处理的Async映射
safeAsyncParMap :: (a -> b) -> [a] -> IO [Either String b]
safeAsyncParMap f xs = do
  asyncs <- mapM (async . safeApply f) xs
  mapM wait asyncs
  where
    safeApply f x = do
      result <- try $ evaluate (f x)
      case result of
        Left e -> return $ Left $ show (e :: SomeException)
        Right r -> return $ Right r

-- 使用示例
demoAsync :: IO ()
demoAsync = do
  let numbers = [1..100000]
  result <- asyncParMap (*2) numbers
  putStrLn $ "Async result: " ++ show (take 10 result)
  
  -- 带错误处理的版本
  safeResult <- safeAsyncParMap (\x -> if x `mod` 1000 == 0 
                                      then error "Simulated error" 
                                      else x * 2) numbers
  putStrLn $ "Safe async result: " ++ show (take 10 safeResult)
```

## Rust实现

### 基础并行映射

```rust
use rayon::prelude::*;
use std::thread;
use std::sync::{Arc, Mutex};

// 基础并行映射
fn par_map<T, R, F>(data: Vec<T>, f: F) -> Vec<R>
where
    T: Send + Sync,
    R: Send,
    F: Fn(&T) -> R + Send + Sync,
{
    data.par_iter().map(f).collect()
}

// 分块并行映射
fn chunked_par_map<T, R, F>(data: Vec<T>, chunk_size: usize, f: F) -> Vec<R>
where
    T: Send + Sync,
    R: Send,
    F: Fn(&T) -> R + Send + Sync,
{
    data.chunks(chunk_size)
        .par_iter()
        .flat_map(|chunk| chunk.iter().map(&f))
        .collect()
}

// 使用示例
fn main() {
    let numbers: Vec<i32> = (1..=1000000).collect();
    
    // 基础并行映射
    let result = par_map(numbers.clone(), |x| x * 2);
    println!("Parallel map result: {:?}", &result[..10]);
    
    // 分块并行映射
    let chunked_result = chunked_par_map(numbers, 1000, |x| x * 2);
    println!("Chunked parallel map result: {:?}", &chunked_result[..10]);
}
```

### 使用线程池的并行映射

```rust
use std::sync::{Arc, Mutex};
use std::thread;
use std::sync::mpsc;

// 线程池并行映射
fn thread_pool_par_map<T, R, F>(data: Vec<T>, num_threads: usize, f: F) -> Vec<R>
where
    T: Send + 'static,
    R: Send + 'static,
    F: Fn(T) -> R + Send + Sync + 'static,
{
    let (tx, rx) = mpsc::channel();
    let data = Arc::new(data);
    let mut handles = vec![];
    
    for thread_id in 0..num_threads {
        let data = Arc::clone(&data);
        let tx = tx.clone();
        let f = &f;
        
        let handle = thread::spawn(move || {
            let chunk_size = data.len() / num_threads;
            let start = thread_id * chunk_size;
            let end = if thread_id == num_threads - 1 {
                data.len()
            } else {
                (thread_id + 1) * chunk_size
            };
            
            for i in start..end {
                if let Some(item) = data.get(i) {
                    let result = f(item.clone());
                    tx.send((i, result)).unwrap();
                }
            }
        });
        
        handles.push(handle);
    }
    
    // 等待所有线程完成
    for handle in handles {
        handle.join().unwrap();
    }
    
    // 收集结果
    let mut results: Vec<Option<R>> = vec![None; data.len()];
    for (index, result) in rx {
        results[index] = Some(result);
    }
    
    results.into_iter().filter_map(|r| r).collect()
}

// 带错误处理的并行映射
fn safe_par_map<T, R, F, E>(data: Vec<T>, f: F) -> Vec<Result<R, E>>
where
    T: Send + Sync,
    R: Send,
    E: Send,
    F: Fn(&T) -> Result<R, E> + Send + Sync,
{
    data.par_iter().map(f).collect()
}

// 使用示例
fn demo_thread_pool() {
    let numbers: Vec<i32> = (1..=100000).collect();
    
    let result = thread_pool_par_map(numbers, 4, |x| x * 2);
    println!("Thread pool result: {:?}", &result[..10]);
    
    // 带错误处理的版本
    let safe_result = safe_par_map(numbers, |x| {
        if x % 1000 == 0 {
            Err("Simulated error")
        } else {
            Ok(x * 2)
        }
    });
    
    println!("Safe parallel map result: {:?}", &safe_result[..10]);
}
```

### 异步并行映射

```rust
use tokio::task;
use futures::future::join_all;

// 异步并行映射
async fn async_par_map<T, R, F>(data: Vec<T>, f: F) -> Vec<R>
where
    T: Send + 'static,
    R: Send + 'static,
    F: Fn(T) -> R + Send + Sync + 'static,
{
    let futures: Vec<_> = data.into_iter()
        .map(|item| {
            let f = &f;
            task::spawn(async move { f(item) })
        })
        .collect();
    
    let results = join_all(futures).await;
    results.into_iter().map(|r| r.unwrap()).collect()
}

// 分块异步映射
async fn chunked_async_par_map<T, R, F>(
    data: Vec<T>, 
    chunk_size: usize, 
    f: F
) -> Vec<R>
where
    T: Send + 'static,
    R: Send + 'static,
    F: Fn(T) -> R + Send + Sync + 'static,
{
    let chunks: Vec<Vec<T>> = data.chunks(chunk_size)
        .map(|chunk| chunk.to_vec())
        .collect();
    
    let futures: Vec<_> = chunks.into_iter()
        .map(|chunk| {
            let f = &f;
            task::spawn(async move {
                chunk.into_iter().map(f).collect::<Vec<R>>()
            })
        })
        .collect();
    
    let chunk_results = join_all(futures).await;
    chunk_results.into_iter()
        .map(|r| r.unwrap())
        .flatten()
        .collect()
}

// 使用示例
#[tokio::main]
async fn demo_async() {
    let numbers: Vec<i32> = (1..=100000).collect();
    
    let result = async_par_map(numbers.clone(), |x| x * 2).await;
    println!("Async parallel map result: {:?}", &result[..10]);
    
    let chunked_result = chunked_async_par_map(numbers, 1000, |x| x * 2).await;
    println!("Chunked async result: {:?}", &chunked_result[..10]);
}
```

## Lean实现

### 依赖类型并行映射

```lean
-- 并行映射类型
structure ParallelMap (α β : Type) where
  map : List α → (α → β) → List β
  isParallel : Prop
  deriving Repr

-- 基础并行映射实现
def basicParallelMap {α β : Type} : ParallelMap α β :=
  ParallelMap.mk 
    (λ xs f => xs.map f)
    (True)

-- 分块并行映射
def chunkedParallelMap {α β : Type} (chunkSize : Nat) : ParallelMap α β :=
  ParallelMap.mk 
    (λ xs f => 
      let chunks := xs.chunk chunkSize
      chunks.bind (λ chunk => chunk.map f))
    (chunkSize > 0)

-- 验证并行映射的正确性
theorem parallel_map_correct {α β : Type} 
  (pm : ParallelMap α β) 
  (xs : List α) 
  (f : α → β) :
  pm.map xs f = xs.map f := by
  cases pm
  simp [ParallelMap.map, List.map]

-- 使用示例
def demo : IO Unit := do
  let numbers := List.range 1000
  let pm := basicParallelMap
  let result := pm.map numbers (λ x => x * 2)
  IO.println s!"Parallel map result: {result.take 10}"
```

### 形式化验证的并行映射

```lean
-- 并行映射规格
structure ParallelMapSpec (α β : Type) where
  inputType : Type
  outputType : Type
  invariant : List α → Prop
  postcondition : List α → (α → β) → List β → Prop
  deriving Repr

-- 验证并行映射
class ValidatedParallelMap (α β : Type) (spec : ParallelMapSpec α β) where
  map : List α → (α → β) → List β
  preservesInvariant : ∀ (xs : List α) (f : α → β),
    spec.invariant xs → spec.invariant xs
  satisfiesPostcondition : ∀ (xs : List α) (f : α → β),
    spec.invariant xs → spec.postcondition xs f (map xs f)

-- 具体并行映射规格
def intParallelMapSpec : ParallelMapSpec Nat Nat :=
  ParallelMapSpec.mk 
    Nat 
    Nat 
    (λ xs => xs.length > 0)
    (λ xs f result => result.length = xs.length ∧ 
                      ∀ i, i < xs.length → result.get? i = some (f (xs.get? i).get))

-- 验证并行映射实现
instance : ValidatedParallelMap Nat Nat intParallelMapSpec where
  map xs f := xs.map f
  preservesInvariant xs f h := h
  satisfiesPostcondition xs f h := by
    simp [intParallelMapSpec, map]
    constructor
    · simp [List.length_map]
    · intro i hi
      simp [List.get?_map, List.get?_eq_get]

-- 证明：并行映射保持长度
theorem parallel_map_preserves_length 
  (xs : List Nat) 
  (f : Nat → Nat) 
  (h : intParallelMapSpec.invariant xs) :
  (ValidatedParallelMap.map xs f).length = xs.length := by
  have := ValidatedParallelMap.satisfiesPostcondition xs f h
  simp [intParallelMapSpec] at this
  exact this.left

-- 使用示例
def demoValidated : IO Unit := do
  let xs := [1, 2, 3, 4, 5]
  
  if intParallelMapSpec.invariant xs then
    let result := ValidatedParallelMap.map xs (λ x => x * 2)
    IO.println s!"Input: {xs}"
    IO.println s!"Output: {result}"
    IO.println "Postcondition satisfied!"
  else
    IO.println "Invalid input"
```

### 函数式并行映射

```lean
-- 函数式并行映射
structure FunctionalParallelMap (α β : Type) where
  map : (α → β) → List α → List β
  deriving Repr

-- 并行映射组合
def composeParallelMaps {α β γ : Type} 
  (f : FunctionalParallelMap β γ) 
  (g : FunctionalParallelMap α β) : 
  FunctionalParallelMap α γ :=
  FunctionalParallelMap.mk (λ h xs => f.map (g.map h) xs)

-- 身份并行映射
def idParallelMap {α : Type} : FunctionalParallelMap α α :=
  FunctionalParallelMap.mk (λ f xs => xs.map f)

-- 使用示例
def demoFunctional : IO Unit := do
  -- 创建并行映射
  let pm1 := FunctionalParallelMap.mk (λ f xs => xs.map f)
  let pm2 := FunctionalParallelMap.mk (λ f xs => xs.map (λ x => f x + 1))
  
  -- 组合并行映射
  let combined := composeParallelMaps pm2 pm1
  
  let numbers := [1, 2, 3, 4, 5]
  let result := combined.map (λ x => x * 2) numbers
  
  IO.println s!"Input: {numbers}"
  IO.println s!"Result: {result}"
```

## 模式对比分析

### 语言特性对比

| 特性 | Haskell | Rust | Lean |
|------|---------|------|------|
| 类型安全 | 强类型系统 | 所有权系统 | 依赖类型 |
| 并行支持 | STM/Par Monad | Rayon/线程池 | 有限支持 |
| 内存管理 | GC | 所有权 | GC |
| 形式验证 | QuickCheck | 有限验证 | 完整证明 |
| 性能 | 惰性求值 | 零成本抽象 | 编译时优化 |

### 实现复杂度

1. **Haskell**: 中等复杂度，Par Monad提供优雅的并行抽象
2. **Rust**: 较高复杂度，需要处理所有权和生命周期
3. **Lean**: 最高复杂度，但提供最强的形式化保证

### 适用场景

- **Haskell**: 函数式编程，数据并行，原型开发
- **Rust**: 系统编程，高性能应用，内存安全要求高
- **Lean**: 形式化验证，数学证明，关键系统

## 最佳实践

### 1. 负载均衡

- 根据数据特性选择合适的分块大小
- 考虑处理器的缓存特性
- 避免数据倾斜

### 2. 错误处理

- Haskell: 使用Either类型和异常处理
- Rust: 使用Result类型和错误传播
- Lean: 使用Option类型和证明

### 3. 性能优化

- Haskell: 使用严格性注解，避免惰性求值开销
- Rust: 使用零拷贝，内联优化
- Lean: 编译时优化，依赖类型消除

## 总结

并行映射模式在不同语言中展现出不同的实现特点：

- **Haskell** 通过Par Monad提供了优雅的并行抽象和强大的类型安全
- **Rust** 通过Rayon和线程池确保了内存安全和高效的性能
- **Lean** 通过依赖类型系统提供了最强的形式化保证和验证能力

选择哪种实现方式取决于具体的应用场景、性能要求和安全性需求。
