# 优化算法 - Haskell实现

## 概述

优化算法是寻找问题最优解的重要工具，广泛应用于机器学习、工程设计、金融建模等领域。本文档使用Haskell实现常见的优化算法，展示函数式编程在优化问题中的优势。

## 遗传算法

### 基本遗传算法框架

```haskell
-- 个体表示
data Individual a = Individual 
    { chromosome :: [a]
    , fitness :: Double
    } deriving (Show, Eq)

-- 遗传算法参数
data GeneticParams = GeneticParams
    { populationSize :: Int
    , mutationRate :: Double
    , crossoverRate :: Double
    , generations :: Int
    } deriving Show

-- 遗传算法类型类
class (Show a, Eq a) => Gene a where
    randomGene :: IO a
    mutate :: a -> IO a
    crossover :: a -> a -> IO (a, a)

-- 遗传算法主函数
geneticAlgorithm :: Gene a => GeneticParams -> IO [a] -> ([a] -> Double) -> IO [a]
geneticAlgorithm params randomIndividual fitnessFunc = do
    -- 初始化种群
    population <- replicateM (populationSize params) randomIndividual
    let initialPop = map (\chrom -> Individual chrom (fitnessFunc chrom)) population
    
    -- 进化过程
    finalPop <- evolve params fitnessFunc initialPop
    
    -- 返回最优个体
    let best = maximumBy (comparing fitness) finalPop
    return $ chromosome best

-- 进化过程
evolve :: Gene a => GeneticParams -> ([a] -> Double) -> [Individual a] -> IO [Individual a]
evolve params fitnessFunc population = 
    let loop gen currentPop = 
            if gen >= generations params
            then return currentPop
            else do
                newPop <- generationStep params fitnessFunc currentPop
                loop (gen + 1) newPop
    in loop 0 population

-- 单代进化
generationStep :: Gene a => GeneticParams -> ([a] -> Double) -> [Individual a] -> IO [Individual a]
generationStep params fitnessFunc population = do
    -- 选择
    parents <- selection population
    
    -- 交叉
    offspring <- crossoverStep params parents
    
    -- 变异
    mutated <- mutationStep params offspring
    
    -- 计算适应度
    let newPopulation = map (\chrom -> Individual chrom (fitnessFunc chrom)) mutated
    
    -- 精英保留
    return $ elitism population newPopulation (populationSize params)

-- 选择操作
selection :: [Individual a] -> IO [Individual a]
selection population = do
    let totalFitness = sum $ map fitness population
        probabilities = map (\ind -> fitness ind / totalFitness) population
    
    -- 轮盘赌选择
    selected <- replicateM (length population) $ rouletteWheel population probabilities
    return selected

-- 轮盘赌选择
rouletteWheel :: [Individual a] -> [Double] -> IO (Individual a)
rouletteWheel population probabilities = do
    r <- randomRIO (0, 1)
    let cumulative = scanl1 (+) probabilities
        selectedIndex = length $ takeWhile (< r) cumulative
    return $ population !! min selectedIndex (length population - 1)

-- 交叉操作
crossoverStep :: Gene a => GeneticParams -> [Individual a] -> IO [[a]]
crossoverStep params parents = do
    let pairs = chunksOf 2 parents
    concatMapM (\(p1:p2:_) -> do
        if randomRIO (0, 1) < crossoverRate params
            then do
                (c1, c2) <- crossover (chromosome p1 !! 0) (chromosome p2 !! 0)
                return [[c1], [c2]]
            else return [chromosome p1, chromosome p2]
        ) pairs

-- 变异操作
mutationStep :: Gene a => GeneticParams -> [[a]] -> IO [[a]]
mutationStep params chromosomes = 
    mapM (\chrom -> 
        mapM (\gene -> 
            if randomRIO (0, 1) < mutationRate params
                then mutate gene
                else return gene
            ) chrom
        ) chromosomes

-- 精英保留
elitism :: [Individual a] -> [Individual a] -> Int -> [Individual a]
elitism oldPop newPop size = 
    let sortedOld = sortBy (flip compare `on` fitness) oldPop
        sortedNew = sortBy (flip compare `on` fitness) newPop
        eliteCount = size `div` 10  -- 保留10%的精英
    in take eliteCount sortedOld ++ take (size - eliteCount) sortedNew
```

### 具体应用：旅行商问题

```haskell
-- 城市表示
data City = City 
    { cityId :: Int
    , x :: Double
    , y :: Double
    } deriving (Show, Eq)

-- 路径表示
type Path = [Int]

-- 计算路径长度
pathLength :: [City] -> Path -> Double
pathLength cities path = 
    let cityList = map (\id -> cities !! id) path
        pairs = zip cityList (tail cityList ++ [head cityList])
    in sum $ map (\(c1, c2) -> sqrt ((x c1 - x c2)^2 + (y c1 - y c2)^2)) pairs

-- 城市基因实例
instance Gene Int where
    randomGene = randomRIO (0, 9)  -- 假设有10个城市
    mutate gene = do
        newGene <- randomRIO (0, 9)
        return newGene
    crossover gene1 gene2 = do
        -- 简单的单点交叉
        return (gene1, gene2)

-- 旅行商问题的遗传算法
tspGeneticAlgorithm :: [City] -> IO Path
tspGeneticAlgorithm cities = do
    let params = GeneticParams 
            { populationSize = 100
            , mutationRate = 0.01
            , crossoverRate = 0.8
            , generations = 1000
            }
        fitnessFunc path = 1 / (1 + pathLength cities path)
        randomPath = shuffle [0..length cities - 1]
    
    result <- geneticAlgorithm params randomPath fitnessFunc
    return result

-- 随机打乱列表
shuffle :: [a] -> IO [a]
shuffle xs = do
    let n = length xs
    indices <- replicateM n $ randomRIO (0, n-1)
    return $ map (xs !!) indices
```

## 模拟退火算法

### 基本框架

```haskell
-- 模拟退火参数
data SimulatedAnnealingParams = SimulatedAnnealingParams
    { initialTemperature :: Double
    , coolingRate :: Double
    , minTemperature :: Double
    , iterationsPerTemp :: Int
    } deriving Show

-- 模拟退火算法
simulatedAnnealing :: (Show a, Eq a) => 
    SimulatedAnnealingParams -> 
    a -> 
    (a -> IO a) -> 
    (a -> Double) -> 
    IO a
simulatedAnnealing params initialSolution neighborFunc costFunc = do
    let loop current temp best bestCost = 
            if temp < minTemperature params
            then return best
            else do
                (newCurrent, newBest, newBestCost) <- 
                    temperatureStep params current temp best bestCost neighborFunc costFunc
                let newTemp = temp * coolingRate params
                loop newCurrent newTemp newBest newBestCost
    
    initialCost <- costFunc initialSolution
    loop initialSolution (initialTemperature params) initialSolution initialCost

-- 单温度步骤
temperatureStep :: (Show a, Eq a) => 
    SimulatedAnnealingParams -> 
    a -> 
    Double -> 
    a -> 
    Double -> 
    (a -> IO a) -> 
    (a -> Double) -> 
    IO (a, a, Double)
temperatureStep params current temp best bestCost neighborFunc costFunc = 
    let loop iter current' best' bestCost' = 
            if iter >= iterationsPerTemp params
            then return (current', best', bestCost')
            else do
                neighbor <- neighborFunc current'
                neighborCost <- costFunc neighbor
                
                let deltaE = neighborCost - costFunc current'
                    acceptance = if deltaE < 0 
                                then 1.0 
                                else exp (-deltaE / temp)
                
                accept <- randomRIO (0, 1)
                let (newCurrent, newBest, newBestCost) = 
                        if accept < acceptance
                        then (neighbor, 
                              if neighborCost < bestCost' then neighbor else best',
                              min neighborCost bestCost')
                        else (current', best', bestCost')
                
                loop (iter + 1) newCurrent newBest newBestCost
    in loop 0 current best bestCost
```

### 应用：函数优化

```haskell
-- 函数优化
optimizeFunction :: (Double -> Double) -> Double -> Double -> IO Double
optimizeFunction func lowerBound upperBound = do
    let params = SimulatedAnnealingParams
            { initialTemperature = 100.0
            , coolingRate = 0.95
            , minTemperature = 0.01
            , iterationsPerTemp = 100
            }
        initialSolution = (lowerBound + upperBound) / 2
        neighborFunc x = do
            delta <- randomRIO (-1, 1)
            let newX = x + delta
            return $ max lowerBound $ min upperBound newX
        costFunc x = -func x  -- 最大化函数值
    
    result <- simulatedAnnealing params initialSolution neighborFunc costFunc
    return result

-- 测试函数
testFunction :: Double -> Double
testFunction x = sin x * exp (-x/10)

-- 运行优化
runFunctionOptimization :: IO Double
runFunctionOptimization = optimizeFunction testFunction 0 20
```

## 粒子群优化

### 基本框架

```haskell
-- 粒子表示
data Particle = Particle
    { position :: [Double]
    , velocity :: [Double]
    , bestPosition :: [Double]
    , bestFitness :: Double
    } deriving Show

-- PSO参数
data PSOParams = PSOParams
    { particleCount :: Int
    , dimensions :: Int
    , maxIterations :: Int
    , w :: Double  -- 惯性权重
    , c1 :: Double -- 个体学习因子
    , c2 :: Double -- 社会学习因子
    } deriving Show

-- 粒子群优化
particleSwarmOptimization :: PSOParams -> ([Double] -> Double) -> IO [Double]
particleSwarmOptimization params fitnessFunc = do
    -- 初始化粒子群
    particles <- initializeParticles params fitnessFunc
    
    -- 找到全局最优
    let globalBest = maximumBy (comparing bestFitness) particles
        globalBestPos = bestPosition globalBest
    
    -- 迭代优化
    finalParticles <- optimizeLoop params fitnessFunc particles globalBestPos
    
    -- 返回全局最优位置
    let finalGlobalBest = maximumBy (comparing bestFitness) finalParticles
    return $ bestPosition finalGlobalBest

-- 初始化粒子群
initializeParticles :: PSOParams -> ([Double] -> Double) -> IO [Particle]
initializeParticles params fitnessFunc = 
    replicateM (particleCount params) $ do
        pos <- replicateM (dimensions params) $ randomRIO (-10, 10)
        vel <- replicateM (dimensions params) $ randomRIO (-1, 1)
        let fitness = fitnessFunc pos
        return $ Particle pos vel pos fitness

-- 优化循环
optimizeLoop :: PSOParams -> ([Double] -> Double) -> [Particle] -> [Double] -> IO [Particle]
optimizeLoop params fitnessFunc particles globalBestPos = 
    let loop iter currentParticles currentGlobalBest = 
            if iter >= maxIterations params
            then return currentParticles
            else do
                newParticles <- updateParticles params fitnessFunc currentParticles currentGlobalBest
                let newGlobalBest = maximumBy (comparing bestFitness) newParticles
                    newGlobalBestPos = bestPosition newGlobalBest
                loop (iter + 1) newParticles newGlobalBestPos
    in loop 0 particles globalBestPos

-- 更新粒子
updateParticles :: PSOParams -> ([Double] -> Double) -> [Particle] -> [Double] -> IO [Particle]
updateParticles params fitnessFunc particles globalBestPos = 
    mapM (\particle -> updateParticle params fitnessFunc particle globalBestPos) particles

-- 更新单个粒子
updateParticle :: PSOParams -> ([Double] -> Double) -> Particle -> [Double] -> IO Particle
updateParticle params fitnessFunc particle globalBestPos = do
    -- 生成随机数
    r1 <- randomRIO (0, 1)
    r2 <- randomRIO (0, 1)
    
    -- 更新速度
    let newVelocity = zipWith4 (\v p pb gb -> 
            w params * v + 
            c1 params * r1 * (pb - p) + 
            c2 params * r2 * (gb - p)) 
            (velocity particle) 
            (position particle) 
            (bestPosition particle) 
            globalBestPos
    
    -- 更新位置
    let newPosition = zipWith (+) (position particle) newVelocity
    
    -- 计算新适应度
    let newFitness = fitnessFunc newPosition
    
    -- 更新个体最优
    let (newBestPos, newBestFitness) = 
            if newFitness > bestFitness particle
            then (newPosition, newFitness)
            else (bestPosition particle, bestFitness particle)
    
    return $ Particle newPosition newVelocity newBestPos newBestFitness
```

### 应用：函数优化

```haskell
-- 测试函数：Rastrigin函数
rastriginFunction :: [Double] -> Double
rastriginFunction xs = 
    let n = fromIntegral $ length xs
        a = 10.0
        sumTerms = sum $ map (\x -> x^2 - a * cos (2 * pi * x)) xs
    in a * n + sumTerms

-- 运行PSO优化
runPSOOptimization :: IO [Double]
runPSOOptimization = do
    let params = PSOParams
            { particleCount = 30
            , dimensions = 2
            , maxIterations = 1000
            , w = 0.7
            , c1 = 1.5
            , c2 = 1.5
            }
    result <- particleSwarmOptimization params rastriginFunction
    return result
```

## 梯度下降算法

### 基本框架

```haskell
-- 梯度下降参数
data GradientDescentParams = GradientDescentParams
    { learningRate :: Double
    , maxIterations :: Int
    , tolerance :: Double
    } deriving Show

-- 梯度下降算法
gradientDescent :: GradientDescentParams -> 
    [Double] -> 
    ([Double] -> [Double]) -> 
    ([Double] -> Double) -> 
    IO [Double]
gradientDescent params initialPoint gradientFunc costFunc = 
    let loop iter currentPoint = 
            if iter >= maxIterations params
            then return currentPoint
            else do
                let gradient = gradientFunc currentPoint
                    newPoint = zipWith (\x g -> x - learningRate params * g) 
                                       currentPoint gradient
                    costDiff = abs (costFunc newPoint - costFunc currentPoint)
                
                if costDiff < tolerance params
                    then return newPoint
                    else loop (iter + 1) newPoint
    in loop 0 initialPoint
```

### 应用：线性回归

```haskell
-- 线性回归梯度
linearRegressionGradient :: [[Double]] -> [Double] -> [Double] -> [Double]
linearRegressionGradient features targets weights = 
    let m = fromIntegral $ length features
        predictions = map (\feature -> sum $ zipWith (*) feature weights) features
        errors = zipWith (-) targets predictions
        
        gradients = map (\j -> 
            let featureJ = map (!! j) features
                gradient = sum $ zipWith (*) errors featureJ
            in (2 / m) * gradient) [0..length weights - 1]
    in gradients

-- 线性回归成本函数
linearRegressionCost :: [[Double]] -> [Double] -> [Double] -> Double
linearRegressionCost features targets weights = 
    let m = fromIntegral $ length features
        predictions = map (\feature -> sum $ zipWith (*) feature weights) features
        squaredErrors = map (\pred -> (pred - target)^2) $ zip predictions targets
    in (1 / (2 * m)) * sum squaredErrors

-- 运行线性回归
runLinearRegression :: [[Double]] -> [Double] -> IO [Double]
runLinearRegression features targets = do
    let params = GradientDescentParams
            { learningRate = 0.01
            , maxIterations = 1000
            , tolerance = 1e-6
            }
        initialWeights = replicate (length $ head features) 0.0
        gradientFunc = linearRegressionGradient features targets
        costFunc = linearRegressionCost features targets
    
    result <- gradientDescent params initialWeights gradientFunc costFunc
    return result
```

## 实际应用示例

### 投资组合优化

```haskell
-- 投资组合表示
type Portfolio = [Double]  -- 权重向量

-- 计算投资组合收益
portfolioReturn :: [Double] -> [Double] -> Double
portfolioReturn weights returns = 
    sum $ zipWith (*) weights returns

-- 计算投资组合风险
portfolioRisk :: [Double] -> [[Double]] -> Double
portfolioRisk weights covarianceMatrix = 
    let weightedCov = zipWith (\w1 row -> 
            sum $ zipWith (\w2 cov -> w1 * w2 * cov) weights row) 
            weights covarianceMatrix
    in sqrt $ sum weightedCov

-- 夏普比率
sharpeRatio :: [Double] -> [Double] -> [[Double]] -> Double -> Double
sharpeRatio weights returns covariance riskFreeRate = 
    let portfolioRet = portfolioReturn weights returns
        portfolioRisk = portfolioRisk weights covariance
    in (portfolioRet - riskFreeRate) / portfolioRisk

-- 投资组合优化
optimizePortfolio :: [Double] -> [[Double]] -> Double -> IO Portfolio
optimizePortfolio returns covariance riskFreeRate = do
    let params = PSOParams
            { particleCount = 50
            , dimensions = length returns
            , maxIterations = 500
            , w = 0.7
            , c1 = 1.5
            , c2 = 1.5
            }
        fitnessFunc weights = sharpeRatio weights returns covariance riskFreeRate
    
    result <- particleSwarmOptimization params fitnessFunc
    return result
```

### 机器学习超参数优化

```haskell
-- 超参数表示
data HyperParameters = HyperParameters
    { learningRate :: Double
    , batchSize :: Int
    , hiddenLayers :: Int
    , neuronsPerLayer :: Int
    } deriving Show

-- 超参数优化
optimizeHyperParameters :: (HyperParameters -> IO Double) -> IO HyperParameters
optimizeHyperParameters validationFunc = do
    let params = GeneticParams
            { populationSize = 20
            , mutationRate = 0.1
            , crossoverRate = 0.8
            , generations = 50
            }
        randomHyperParams = do
            lr <- randomRIO (0.001, 0.1)
            bs <- randomRIO (16, 256)
            hl <- randomRIO (1, 5)
            npl <- randomRIO (32, 512)
            return $ HyperParameters lr bs hl npl
        
        fitnessFunc hp = do
            score <- validationFunc hp
            return $ 1 / (1 + score)  -- 转换为最大化问题
    
    result <- geneticAlgorithm params randomHyperParams fitnessFunc
    return result
```

## 性能优化

### 并行优化

```haskell
import Control.Parallel.Strategies

-- 并行粒子群优化
parallelPSO :: PSOParams -> ([Double] -> Double) -> IO [Double]
parallelPSO params fitnessFunc = do
    particles <- initializeParticles params fitnessFunc
    
    let parallelUpdate particles globalBest = 
            parMap rpar (\p -> updateParticle params fitnessFunc p globalBest) particles
    
    -- 并行更新所有粒子
    finalParticles <- optimizeLoopParallel params fitnessFunc particles
    let finalGlobalBest = maximumBy (comparing bestFitness) finalParticles
    return $ bestPosition finalGlobalBest

-- 并行优化循环
optimizeLoopParallel :: PSOParams -> ([Double] -> Double) -> [Particle] -> IO [Particle]
optimizeLoopParallel params fitnessFunc particles = 
    let loop iter currentParticles = 
            if iter >= maxIterations params
            then return currentParticles
            else do
                let globalBest = maximumBy (comparing bestFitness) currentParticles
                    globalBestPos = bestPosition globalBest
                
                newParticles <- parallelUpdate currentParticles globalBestPos
                loop (iter + 1) newParticles
    in loop 0 particles
```

### 内存优化

```haskell
-- 流式优化
streamOptimization :: [Double] -> (Double -> Double) -> IO Double
streamOptimization initialValues fitnessFunc = 
    let stream = iterate (\x -> x + 0.01) (head initialValues)
        optimized = map fitnessFunc stream
        best = maximum optimized
    in return best

-- 缓存优化结果
type OptimizationCache = Map String Double

-- 带缓存的优化
cachedOptimization :: OptimizationCache -> String -> IO Double -> IO (Double, OptimizationCache)
cachedOptimization cache key optimizationFunc = 
    case Map.lookup key cache of
        Just result -> return (result, cache)
        Nothing -> do
            result <- optimizationFunc
            let newCache = Map.insert key result cache
            return (result, newCache)
```

## 总结

本文档展示了Haskell中优化算法的完整实现，包括：

1. **遗传算法**：基本框架、旅行商问题应用
2. **模拟退火**：基本框架、函数优化应用
3. **粒子群优化**：基本框架、函数优化应用
4. **梯度下降**：基本框架、线性回归应用
5. **实际应用**：投资组合优化、超参数优化
6. **性能优化**：并行处理、内存优化

这些算法展示了Haskell在优化问题中的优势：

- **类型安全**：编译时保证算法正确性
- **函数式风格**：清晰的算法表达
- **高阶函数**：抽象优化模式
- **不可变性**：避免副作用，便于并行化
- **惰性求值**：优化计算性能

通过这些实现，我们可以看到Haskell不仅适合理论研究，也适合实际的优化问题求解。
